{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/artemiy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/artemiy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/artemiy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Install uncommon packages (will supply requements.txt later)\n",
    "#!pip install git+https://github.com/boudinfl/pke.git\n",
    "#!pip install wordninja\n",
    "#!pip install pyenchant\n",
    "\n",
    "# For macos:\n",
    "#brew install cmake\n",
    "#git clone git@github.com:lloyd/yajl.git\n",
    "#cd yajl\n",
    "#./configure && make install\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import wordninja\n",
    "from os import listdir\n",
    "import re\n",
    "import sys\n",
    "import ast\n",
    "import pke\n",
    "import enchant\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import requests\n",
    "import pickle\n",
    "#import pycorpora\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "d = enchant.Dict(\"en_US\") # ENGLISH Dictionary\n",
    "\n",
    "\n",
    "# CHANGE THE PATH TO A DATASET HERE\n",
    "basedir = os.path.join(os.sep, \"Users\", \"artemiy\", \"Disk-O\", \"tyomkolton@mail.ru-mailru\", \"ML\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded descriptions\n",
      "loaded org infos\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(basedir, \"CB_export\", \"cb_organization_descriptions.csv\"), index_col=\"uuid\")\n",
    "#data.iloc[:,1].to_csv('CB_Export_17_08_07/descr.txt')\n",
    "print(\"loaded descriptions\")\n",
    "data1 = pd.read_csv(os.path.join(basedir, \"CB_export\", \"cb_organizations.csv\"), low_memory=False, index_col=\"uuid\")\n",
    "print(\"loaded org infos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged\n"
     ]
    }
   ],
   "source": [
    "# Merge 2 csv files\n",
    "# we can time to see which is more effective\n",
    "#result = pd.concat([data, data1], axis=1, sort=False)\n",
    "result1 = pd.merge(data, data1, how='inner', left_index=True, right_index=True)\n",
    "print(\"merged\")\n",
    "# Delete all companies that do not have any descriptions\n",
    "result1 = result1.dropna(subset=[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(((result.shape[0] - result1.shape[0])/result.shape[0]*100), \"% of companies do not have any description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>company_name</th>\n",
       "      <th>primary_role</th>\n",
       "      <th>permalink</th>\n",
       "      <th>domain</th>\n",
       "      <th>homepage_url</th>\n",
       "      <th>country_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>phone</th>\n",
       "      <th>facebook_url</th>\n",
       "      <th>linkedin_url</th>\n",
       "      <th>cb_url</th>\n",
       "      <th>logo_url</th>\n",
       "      <th>profile_image_url</th>\n",
       "      <th>twitter_url</th>\n",
       "      <th>alias</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uuid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f5bb580f-d655-cf3f-9ade-eca5b3f2719f</th>\n",
       "      <td>Android, Apple - iOS, Blackberry, Windows Phon...</td>\n",
       "      <td>VilarikA</td>\n",
       "      <td>company</td>\n",
       "      <td>/organization/vilarika</td>\n",
       "      <td>vilarika.com.br</td>\n",
       "      <td>http://vilarika.com.br/</td>\n",
       "      <td>BRA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>Belo Horizonte</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.crunchbase.com/organization/vilarika</td>\n",
       "      <td>https://www.crunchbase.com/organization/vilari...</td>\n",
       "      <td>http://public.crunchbase.com/t_api_images/v141...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-12-11 06:46:05</td>\n",
       "      <td>2016-09-07 00:03:51.67913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000aa4-ba42-9b68-a9c3-040c9f3bf9b9</th>\n",
       "      <td>Formel D GmbH is a automotive manufacturer and...</td>\n",
       "      <td>Formel D GmbH</td>\n",
       "      <td>company</td>\n",
       "      <td>/organization/formel-d-gmbh</td>\n",
       "      <td>formeld.com</td>\n",
       "      <td>http://www.formeld.com</td>\n",
       "      <td>DEU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DEU - Other</td>\n",
       "      <td>Troisdorf</td>\n",
       "      <td>...</td>\n",
       "      <td>+49 2241 9960</td>\n",
       "      <td>https://www.facebook.com/formeld</td>\n",
       "      <td>https://www.linkedin.com/company/formel-d-group</td>\n",
       "      <td>https://www.crunchbase.com/organization/formel...</td>\n",
       "      <td>https://www.crunchbase.com/organization/formel...</td>\n",
       "      <td>http://public.crunchbase.com/t_api_images/v148...</td>\n",
       "      <td>https://www.twitter.com/formeld_es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-06-01 06:58:37.692725</td>\n",
       "      <td>2017-07-18 07:22:19.15864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5bd38d1-7719-2935-fb6c-defae39f5b93</th>\n",
       "      <td>[iNFoGooL](http://infogool.com) - The Informat...</td>\n",
       "      <td>infogool</td>\n",
       "      <td>company</td>\n",
       "      <td>/organization/infogool</td>\n",
       "      <td>infogool.com</td>\n",
       "      <td>http://infogool.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8802393009</td>\n",
       "      <td>http://www.facebook.com/infogool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.crunchbase.com/organization/infogool</td>\n",
       "      <td>https://www.crunchbase.com/organization/infogo...</td>\n",
       "      <td>http://public.crunchbase.com/t_api_images/v139...</td>\n",
       "      <td>https://www.twitter.com/infogool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-04-12 00:10:12</td>\n",
       "      <td>2016-09-08 22:02:05.585875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773adc18-132a-937d-8841-4f833e64dd56</th>\n",
       "      <td>The Jinfeng Gold Mine is an combined open pit ...</td>\n",
       "      <td>Jinfeng Mine</td>\n",
       "      <td>company</td>\n",
       "      <td>/organization/jinfeng-mine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.crunchbase.com/organization/jinfen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-07 06:43:06.955155</td>\n",
       "      <td>2016-12-07 06:45:09.14727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5bdd48a-a09f-2f4c-bd71-4c4207c7731b</th>\n",
       "      <td>Peardoc offers online tools to convert HTML to...</td>\n",
       "      <td>Peardoc Solutions</td>\n",
       "      <td>company</td>\n",
       "      <td>/organization/peardoc-solutions</td>\n",
       "      <td>peardoc.com</td>\n",
       "      <td>http://www.peardoc.com</td>\n",
       "      <td>IND</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.crunchbase.com/organization/peardo...</td>\n",
       "      <td>https://www.crunchbase.com/organization/peardo...</td>\n",
       "      <td>http://public.crunchbase.com/t_api_images/v140...</td>\n",
       "      <td>https://www.twitter.com/pear_doc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-07-17 10:06:03</td>\n",
       "      <td>2016-03-08 02:56:01.230586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            description  \\\n",
       "uuid                                                                                      \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f  Android, Apple - iOS, Blackberry, Windows Phon...   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  Formel D GmbH is a automotive manufacturer and...   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93  [iNFoGooL](http://infogool.com) - The Informat...   \n",
       "773adc18-132a-937d-8841-4f833e64dd56  The Jinfeng Gold Mine is an combined open pit ...   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  Peardoc offers online tools to convert HTML to...   \n",
       "\n",
       "                                           company_name primary_role  \\\n",
       "uuid                                                                   \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f           VilarikA      company   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9      Formel D GmbH      company   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93           infogool      company   \n",
       "773adc18-132a-937d-8841-4f833e64dd56       Jinfeng Mine      company   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  Peardoc Solutions      company   \n",
       "\n",
       "                                                            permalink  \\\n",
       "uuid                                                                    \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f           /organization/vilarika   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9      /organization/formel-d-gmbh   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93           /organization/infogool   \n",
       "773adc18-132a-937d-8841-4f833e64dd56       /organization/jinfeng-mine   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  /organization/peardoc-solutions   \n",
       "\n",
       "                                               domain  \\\n",
       "uuid                                                    \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f  vilarika.com.br   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9      formeld.com   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93     infogool.com   \n",
       "773adc18-132a-937d-8841-4f833e64dd56              NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b      peardoc.com   \n",
       "\n",
       "                                                 homepage_url country_code  \\\n",
       "uuid                                                                         \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f  http://vilarika.com.br/          BRA   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9   http://www.formeld.com          DEU   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93      http://infogool.com          NaN   \n",
       "773adc18-132a-937d-8841-4f833e64dd56                      NaN          CHN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b   http://www.peardoc.com          IND   \n",
       "\n",
       "                                     state_code          region  \\\n",
       "uuid                                                              \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f        NaN  Rio de Janeiro   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9        NaN     DEU - Other   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93        NaN             NaN   \n",
       "773adc18-132a-937d-8841-4f833e64dd56        NaN             NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b        NaN         Chennai   \n",
       "\n",
       "                                                city  \\\n",
       "uuid                                                   \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f  Belo Horizonte   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9       Troisdorf   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93             NaN   \n",
       "773adc18-132a-937d-8841-4f833e64dd56             NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b         Chennai   \n",
       "\n",
       "                                                 ...              \\\n",
       "uuid                                             ...               \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f             ...               \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9             ...               \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93             ...               \n",
       "773adc18-132a-937d-8841-4f833e64dd56             ...               \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b             ...               \n",
       "\n",
       "                                              phone  \\\n",
       "uuid                                                  \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f            NaN   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  +49 2241 9960   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93     8802393009   \n",
       "773adc18-132a-937d-8841-4f833e64dd56            NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b            NaN   \n",
       "\n",
       "                                                          facebook_url  \\\n",
       "uuid                                                                     \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f                               NaN   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  https://www.facebook.com/formeld   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93  http://www.facebook.com/infogool   \n",
       "773adc18-132a-937d-8841-4f833e64dd56                               NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b                               NaN   \n",
       "\n",
       "                                                                         linkedin_url  \\\n",
       "uuid                                                                                    \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f                                              NaN   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  https://www.linkedin.com/company/formel-d-group   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93                                              NaN   \n",
       "773adc18-132a-937d-8841-4f833e64dd56                                              NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b                                              NaN   \n",
       "\n",
       "                                                                                 cb_url  \\\n",
       "uuid                                                                                      \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f   https://www.crunchbase.com/organization/vilarika   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  https://www.crunchbase.com/organization/formel...   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93   https://www.crunchbase.com/organization/infogool   \n",
       "773adc18-132a-937d-8841-4f833e64dd56  https://www.crunchbase.com/organization/jinfen...   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  https://www.crunchbase.com/organization/peardo...   \n",
       "\n",
       "                                                                               logo_url  \\\n",
       "uuid                                                                                      \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f  https://www.crunchbase.com/organization/vilari...   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  https://www.crunchbase.com/organization/formel...   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93  https://www.crunchbase.com/organization/infogo...   \n",
       "773adc18-132a-937d-8841-4f833e64dd56                                                NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  https://www.crunchbase.com/organization/peardo...   \n",
       "\n",
       "                                                                      profile_image_url  \\\n",
       "uuid                                                                                      \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f  http://public.crunchbase.com/t_api_images/v141...   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  http://public.crunchbase.com/t_api_images/v148...   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93  http://public.crunchbase.com/t_api_images/v139...   \n",
       "773adc18-132a-937d-8841-4f833e64dd56                                                NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  http://public.crunchbase.com/t_api_images/v140...   \n",
       "\n",
       "                                                             twitter_url  \\\n",
       "uuid                                                                       \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f                                 NaN   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  https://www.twitter.com/formeld_es   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93    https://www.twitter.com/infogool   \n",
       "773adc18-132a-937d-8841-4f833e64dd56                                 NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b    https://www.twitter.com/pear_doc   \n",
       "\n",
       "                                      alias                  created_at  \\\n",
       "uuid                                                                      \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f    NaN         2014-12-11 06:46:05   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9    NaN  2016-06-01 06:58:37.692725   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93    NaN         2014-04-12 00:10:12   \n",
       "773adc18-132a-937d-8841-4f833e64dd56    NaN  2016-12-07 06:43:06.955155   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b    NaN         2014-07-17 10:06:03   \n",
       "\n",
       "                                                      updated_at  \n",
       "uuid                                                              \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f   2016-09-07 00:03:51.67913  \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9   2017-07-18 07:22:19.15864  \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93  2016-09-08 22:02:05.585875  \n",
       "773adc18-132a-937d-8841-4f833e64dd56   2016-12-07 06:45:09.14727  \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  2016-03-08 02:56:01.230586  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353146"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constuct a list of companies that have descriptions\n",
    "#d_companies_list = list(result1.loc[:, \"company_name\"])\n",
    "#d_companies_list = [company.lower() for company in d_companies_list]\n",
    "#len(d_companies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20249\n"
     ]
    }
   ],
   "source": [
    "# Construct a list of companies from dataset folder\n",
    "f_companies_list = os.listdir(os.path.join(basedir, \"News_1\"))\n",
    "# Get rid of \".json\" extension and apply uppercase\n",
    "f_companies_list = [re.sub(\".json\", \"\", company) for company in f_companies_list]\n",
    "print(len(f_companies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12609\n",
      "11385\n"
     ]
    }
   ],
   "source": [
    "# Get rid of companies with names containing anything else than latin letters (as well as containing \"_\" separating multiple words in names)\n",
    "f_companies_list = [company.lower() for company in f_companies_list if all(letter in string.ascii_letters for letter in company)]\n",
    "print(len(f_companies_list))\n",
    "\n",
    "# Get rid of companies whose full names consist of one single word from english dictionary\n",
    "f_companies_list = [company for company in f_companies_list if not d.check(company.lower())]\n",
    "print(len(f_companies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367\n"
     ]
    }
   ],
   "source": [
    "def word_exists(word, apiKey=apiKey, apiUrl='http://api.wordnik.com/v4'):\n",
    "\n",
    "    # Construct the API URL for a random-word query\n",
    "    api_url = \"{baseurl}/words.json/search/{word}\".format(\n",
    "        baseurl=apiUrl,\n",
    "        word=word\n",
    "    )\n",
    "\n",
    "    parameters = {\n",
    "        'api_key': apiKey,\n",
    "    }\n",
    "\n",
    "    # Perform the query and store the HTTP response object\n",
    "    response = requests.get(api_url, params=parameters)\n",
    "\n",
    "    # Convert the response content to a list\n",
    "    # NOTE: The content is initially returned as a byte string\n",
    "    word_object = json.loads(response.content)\n",
    "\n",
    "    # Get the number word's instances in the dictionary\n",
    "    if word_object:\n",
    "        if word_object['searchResults'][0]['count'] > 0:\n",
    "            result = True\n",
    "        else:\n",
    "            result = False\n",
    "    else:\n",
    "        result = False\n",
    "\n",
    "    # Return the word as a string\n",
    "    return result\n",
    "\n",
    "# Same as the previous step (looking for english words) but with Wordnik API\n",
    "apiUrl = 'http://api.wordnik.com/v4'\n",
    "apiKey = 'b0a856e4cbe5c4e3f110d0527b901b7064f22a1c09e153547'\n",
    "# Extract the list words from Wordnik that DO exist\n",
    "wordnik_companies_list = [company for company in f_companies_list if word_exists(company)]\n",
    "print(len(wordnik_companies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11385\n",
      "11385\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Tries to separate a name string into multiple words and if succeeds gets rid of the company (a lot of false positives)\n",
    "# companies_list = [company for company in companies_list if len(wordninja.split(company.lower())) < 2]\n",
    "# print(len(companies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accend', 'acton', 'aerospike', 'afterschool', 'ais', 'alation', 'alector', 'alignable', 'amberjack', 'amicus', 'ananas', 'ansible', 'anyroad', 'aptera', 'aruspex', 'asana', 'attask', 'auris', 'authorly', 'aviso', 'ayr', 'backchat', 'backdoor', 'backtrace', 'bannerman', 'bastille', 'beme', 'bento', 'bevvy', 'biome', 'biscotti', 'bittorrent', 'blesh', 'bonafide', 'bonobos', 'bookbag', 'boombox', 'booyah', 'bottlenose', 'bowery', 'boxfish', 'brit', 'browster', 'carbonite', 'cardio', 'cartesian', 'cartogram', 'celly', 'centro', 'chargeback', 'chockstone', 'chronicity', 'clickable', 'cloze', 'containership', 'contentful', 'convo', 'copromote', 'courseload', 'courseloads', 'cringle', 'crossfader', 'cuculus', 'dekko', 'dextro', 'digby', 'doo', 'doozie', 'doppelganger', 'driverside', 'dropoff', 'dropship', 'duetto', 'eco', 'elastica', 'elsen', 'emma', 'endorphin', 'endplay', 'enow', 'entrada', 'epiphyte', 'eponym', 'eps', 'ess', 'everly', 'evolv', 'evolver', 'exogenesis', 'eyespot', 'fab', 'facebook', 'fanbase', 'faraday', 'fastly', 'faves', 'figma', 'figo', 'firebase', 'firethorn', 'fitt', 'fixt', 'flashpoint', 'flavour', 'flite', 'fluential', 'fluther', 'flyer', 'fogger', 'fondu', 'foodist', 'fora', 'forerun', 'fulham', 'fundation', 'fuze', 'fy', 'getable', 'gett', 'globality', 'glyde', 'goby', 'goji', 'goot', 'grabble', 'graphicly', 'greenling', 'gridpoint', 'hability', 'hallux', 'hangtime', 'happify', 'hashable', 'headnote', 'headspace', 'higgle', 'highspot', 'homie', 'hotlink', 'hotlist', 'housecall', 'huckle', 'hux', 'hyr', 'igg', 'ilike', 'inlist', 'innit', 'insense', 'inturn', 'invision', 'iterable', 'jobble', 'jobster', 'jumpcut', 'jumpshot', 'kaboodle', 'kahuna', 'kapow', 'kapta', 'kast', 'kat', 'keas', 'kensho', 'kickboard', 'kickup', 'kiva', 'knod', 'kudo', 'kuna', 'kyte', 'lanx', 'larky', 'leet', 'legalist', 'levo', 'lexigram', 'lifecycle', 'lifesize', 'lifestreams', 'lightswitch', 'lima', 'lookit', 'loup', 'loupe', 'loveseat', 'luma', 'lux', 'lyft', 'mahalo', 'makara', 'markhor', 'masala', 'mashable', 'matcha', 'matchpoint', 'mately', 'mediant', 'medio', 'meetup', 'melba', 'metadata', 'milo', 'miso', 'mobee', 'mochila', 'mog', 'moki', 'moli', 'mozy', 'mux', 'myspace', 'nav', 'netbooks', 'netminder', 'nextdoor', 'nitch', 'nitro', 'noesis', 'noke', 'nom', 'nota', 'nucleonics', 'oculus', 'offline', 'ogin', 'ollie', 'ombu', 'onefold', 'onlive', 'ono', 'oratio', 'orca', 'ossia', 'ouroboros', 'outro', 'overdog', 'overwatch', 'parsable', 'payscale', 'peloton', 'perq', 'petrichor', 'phenom', 'phil', 'phononic', 'pickie', 'pieris', 'pinscreen', 'pipefish', 'playdate', 'playlist', 'plex', 'pley', 'pluot', 'pomello', 'popin', 'popup', 'poshly', 'powerset', 'poynt', 'precog', 'pureplay', 'qualia', 'quartzy', 'quora', 'ramen', 'rebit', 'recurve', 'redd', 'redfin', 'redox', 'redux', 'remerge', 'repp', 'revaluate', 'revolv', 'ribbit', 'ridley', 'rize', 'rowl', 'rythm', 'saleswise', 'samsara', 'screenie', 'seaters', 'seeme', 'senet', 'sensoria', 'sente', 'serica', 'serverless', 'shapeup', 'shippo', 'shoutout', 'sinch', 'skift', 'skout', 'skully', 'smore', 'smyte', 'snapt', 'sofi', 'soko', 'solum', 'sonation', 'sonder', 'speek', 'splurgy', 'spoonflower', 'stickybeak', 'stormwind', 'stringify', 'strix', 'superfly', 'superphone', 'supersecret', 'talkable', 'tastebud', 'tastemaker', 'technorati', 'telematic', 'teleport', 'thanx', 'thru', 'thryve', 'tidemark', 'tidepool', 'timeful', 'tock', 'toro', 'tred', 'treehouse', 'trippy', 'trover', 'twelvefold', 'tyche', 'tympany', 'uber', 'umami', 'understory', 'unmute', 'upto', 'uru', 'vayu', 'vera', 'voxel', 'vroom', 'vue', 'waldo', 'wasabi', 'wavemaker', 'werk', 'wetpaint', 'whiptail', 'wildcard', 'wiselike', 'wize', 'wunder', 'wut', 'wynk', 'xing', 'yantra', 'yarly', 'yola', 'yowza', 'zafu']\n"
     ]
    }
   ],
   "source": [
    "# These words DO exist in Wordnik dictionary\n",
    "print((wordnik_companies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use \n",
    "if not os.path.exists(os.path.join(basedir, 'temp_data')):\n",
    "    os.makedirs(os.path.join(basedir, 'temp_data'))\n",
    "with open('wordnik_companies_list.txt', 'wb') as fp:\n",
    "    pickle.dump(wordnik_companies_list, fp)\n",
    "with open ('wordnik_companies_list.txt', 'rb') as fp:\n",
    "    wordnik_companies_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11018\n",
      "['aaptiv', 'abaris', 'abbeypost', 'abcmob', 'abeo', 'abililife', 'abilto', 'abine', 'abiobot', 'ablexis', 'ablio', 'abodo', 'abogen', 'abom', 'aboutlife', 'aboutme', 'aboutone', 'aboutusorg', 'abra', 'absci', 'absmaterials', 'abusix', 'abvitro', 'abyrx', 'academiaedu', 'academixdirect', 'acadiasoft', 'accelera', 'accelereach', 'accelergy', 'accelgolf', 'accellos', 'accelo', 'accelops', 'acceptd', 'accera', 'accertify', 'accesssportsmediacom', 'acclaimd', 'acclarent', 'accountnow', 'accredible', 'accreon', 'accuitis', 'acculitx', 'accumen', 'accuradio', 'accuvein', 'accuwater', 'aceable', 'acebotai', 'acertiv', 'achaogen', 'achieveit', 'acompli', 'acopio', 'acousticeye', 'acquaintable', 'acquia', 'acrinta', 'acrisure', 'acrobatiq', 'acronis', 'acsian', 'actacell', 'actifio', 'actionality', 'actionsprout', 'actionx', 'activaero', 'actived', 'activegrid', 'activehours', 'activepath', 'activeprotective', 'activerain', 'activiter', 'activityhero', 'actmd', 'actualmeds', 'actuatedmedical', 'actx', 'acufocus', 'acumatica', 'acumera', 'acupera', 'adadapted', 'adadyn', 'adallom', 'adapteva', 'adaptiveblue', 'adaptly', 'adaptv', 'adara', 'adavium', 'adayana', 'adbrite', 'adcade', 'adchemy', 'addapp']\n"
     ]
    }
   ],
   "source": [
    "f_companies_list = [e for e in f_companies_list if e not in set(wordnik_companies_list)]\n",
    "print(len(f_companies_list))\n",
    "print(f_companies_list[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345890"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data frame of companies descriptions to be used in final data frame with articles\n",
    "d_companies_df = result1\n",
    "d_companies_df.loc[:,'company_name'] = d_companies_df.loc[:,'company_name'].str.lower()\n",
    "#d_companies_df = d_companies_df[d_companies_df['company_name'].str.lower().isin(d_companies_list)]\n",
    "#d_companies_df = d_companies_df[d_companies_df['company_name'].isin(d_companies_list)]\n",
    "d_companies_df = d_companies_df.reset_index().set_index(keys='company_name')#, verify_integrity=True)\n",
    "\n",
    "\n",
    "\n",
    "# Delete the companies which have the same name\n",
    "d_companies_df = d_companies_df[~d_companies_df.index.duplicated(keep=False)]\n",
    "\n",
    "\n",
    "d_companies_df = d_companies_df[['uuid', 'description']]\n",
    "len(d_companies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9658\n"
     ]
    }
   ],
   "source": [
    "# Delete companies outside or dataset of articles\n",
    "companies_df = d_companies_df[d_companies_df.index.isin(f_companies_list)]\n",
    "companies_list = companies_df.index.values\n",
    "print(len(companies_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use with keywords extraction\n",
    "if not os.path.exists(os.path.join(basedir, 'temp_data')):\n",
    "    os.makedirs(os.path.join(basedir, 'temp_data'))\n",
    "with open('companies_df.csv', 'wb') as fp:\n",
    "    pickle.dump(companies_df, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9658\n"
     ]
    }
   ],
   "source": [
    "with open ('companies_df.csv', 'rb') as fp:\n",
    "    companies_df = pickle.load(fp)\n",
    "companies_df.head()\n",
    "companies_list = companies_df.index.values\n",
    "print(len(companies_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/artemiy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ONE WAY TO EXTRACT KEYWORDS ###\n",
    "\n",
    "# With PKE Package\n",
    "tmp = companies_df\n",
    "\n",
    "# initialize keyphrase extraction model, here TopicRank\n",
    "import pke\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description for company ineomarketing (954b5316-3bcc-6081-d018-fec2c374d395) is too short. Description: ineomarketing\n",
      "Description for company arrowsight (23bea55a-2e83-2b39-b53a-d8bd28d218e8) is too short. Description: arrowsight\n",
      "Description for company looklive (6d3b1bed-302a-3331-6f43-87f91ca6a54b) is too short. Description: looklive\n",
      "Description for company cityspark (f25f78b7-1a86-bf9d-e5da-24a538b81c61) is too short. Description: cityspark\n",
      "Description for company rentabilities (aa8e1403-1db8-e5bf-6b5c-423bb826ccfd) is too short. Description: rentabilities\n",
      "Description for company redkix (0510d73a-8709-bcae-5930-f047ee8b9db7) is too short. Description: redkix\n",
      "Description for company shaser (538439c3-693e-0c54-76e3-f6c4c237370c) is too short. Description: shaser\n",
      "Description for company bamboostr (83ccded4-e8ca-782f-2ef6-2c5b020fe48a) is too short. Description: bamboostr\n",
      "Description for company metacert (83d3852a-f60d-2bac-24fe-afdbfb316fe9) is too short. Description: metacert\n",
      "Description for company wesabe (697df046-5264-0394-1ff2-7109a0d6e848) is too short. Description: wesabe\n",
      "Description for company emms (4eeacc81-59cd-5d60-e32b-313d6d0e9d9a) is too short. Description: emms\n",
      "Description for company xquva (704b794a-03b3-9d70-52d2-3175b37915a9) is too short. Description: xquva\n",
      "Description for company niftythrifty (6244d835-00b6-657d-ff76-bf1c364e58c2) is too short. Description: niftythrifty\n",
      "Description for company liquidlandscape (d9006263-2c1a-5f07-7fdf-7514bc200889) is too short. Description: liquidlandscape\n",
      "Description for company novogy (eb26bec8-dc57-caca-9aef-fdf82ff29505) is too short. Description: novogy\n",
      "Description for company quickcoin (508fd8ff-9262-5ee4-f9da-2eb2fa6d903c) is too short. Description: quickcoin\n",
      "Description for company strivewire (fc09016f-8f99-d862-80fb-77abfe49f335) is too short. Description: strivewire\n",
      "Description for company ventealapropriete (34f1b9aa-d217-947c-44b7-f57c1a323522) is too short. Description: ventealapropriete\n",
      "Description for company backfence (b18646cf-629e-f95a-e2d2-ef46e8a4288e) is too short. Description: backfence\n",
      "Description for company whelse (419bfe10-233d-e584-8b88-c02ffdc964dc) is too short. Description: whelse\n",
      "Description for company aevena (d389008e-b44a-c475-d82f-60f80d58bff1) is too short. Description: aevena\n",
      "Description for company diassess (57198738-43d7-739a-67ec-9c3bef542218) is too short. Description: diassess\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>description</th>\n",
       "      <th>kp_topicrank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hooja</th>\n",
       "      <td>ff71401a-f5b0-1dd8-2b72-4a9e5d6ecba0</td>\n",
       "      <td>Hooja is a company focused on the mobile realm...</td>\n",
       "      <td>[('service', 0.0785488995371759), ('hooja', 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseclick</th>\n",
       "      <td>d140620f-09bb-dd33-baaa-ecffff1d8bc6</td>\n",
       "      <td>baseclick GmbH is a start-up company funded an...</td>\n",
       "      <td>[('technologies', 0.10249367027116733), ('star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biodetego</th>\n",
       "      <td>fef487a3-2505-e176-7bdc-d82db18525dc</td>\n",
       "      <td>BioDetego is developing, VASPfore, a new bioma...</td>\n",
       "      <td>[('cancer treatment protocols', 0.105445975900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>socialthreader</th>\n",
       "      <td>ffe12892-05c4-e8e4-d728-d9b54e7b3aeb</td>\n",
       "      <td>SocialThreader increases effectiveness of digi...</td>\n",
       "      <td>[('campaign performance', 0.1546622347489285),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>findally</th>\n",
       "      <td>cc693948-abf3-e3dc-f845-3e38ac718aab</td>\n",
       "      <td>Findally is an image-based product engine that...</td>\n",
       "      <td>[('image-based product engine', 0.168320437934...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                uuid  \\\n",
       "company_name                                           \n",
       "hooja           ff71401a-f5b0-1dd8-2b72-4a9e5d6ecba0   \n",
       "baseclick       d140620f-09bb-dd33-baaa-ecffff1d8bc6   \n",
       "biodetego       fef487a3-2505-e176-7bdc-d82db18525dc   \n",
       "socialthreader  ffe12892-05c4-e8e4-d728-d9b54e7b3aeb   \n",
       "findally        cc693948-abf3-e3dc-f845-3e38ac718aab   \n",
       "\n",
       "                                                      description  \\\n",
       "company_name                                                        \n",
       "hooja           Hooja is a company focused on the mobile realm...   \n",
       "baseclick       baseclick GmbH is a start-up company funded an...   \n",
       "biodetego       BioDetego is developing, VASPfore, a new bioma...   \n",
       "socialthreader  SocialThreader increases effectiveness of digi...   \n",
       "findally        Findally is an image-based product engine that...   \n",
       "\n",
       "                                                     kp_topicrank  \n",
       "company_name                                                       \n",
       "hooja           [('service', 0.0785488995371759), ('hooja', 0....  \n",
       "baseclick       [('technologies', 0.10249367027116733), ('star...  \n",
       "biodetego       [('cancer treatment protocols', 0.105445975900...  \n",
       "socialthreader  [('campaign performance', 0.1546622347489285),...  \n",
       "findally        [('image-based product engine', 0.168320437934...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "for index, row in companies_df.iterrows():\n",
    "    \n",
    "    \n",
    "    counter +=1\n",
    "    #if counter >3:\n",
    "    #    break\n",
    "    \n",
    "    extractor = pke.unsupervised.TopicRank()\n",
    "    # load the content of the document, here document is expected to be in raw\n",
    "    # format (i.e. a simple string of text) and preprocessing is carried out using nltk\n",
    "    s = row[\"description\"]\n",
    "    #s = ' '.join(s.split())\n",
    "    #s = \"fwfewk w31231dmm! test test hello you piece of mind balls\"\n",
    "    extractor.read_text(s)\n",
    "\n",
    "    # keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
    "    # and adjectives\n",
    "    extractor.candidate_selection()\n",
    "\n",
    "    # candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "    try:\n",
    "        extractor.candidate_weighting()\n",
    "        # N-best selection, keyphrases contains the 10 highest scored candidates as\n",
    "        # (keyphrase, score) tuples\n",
    "        keyphrases = extractor.get_n_best(n=10, stemming=False)\n",
    "        keyphrases = sorted(keyphrases, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        tmp.loc[index, 'kp_topicrank'] = str(keyphrases)\n",
    "    \n",
    "    except ValueError:\n",
    "        print(\"Description for company \" + index + \n",
    "              \" (\" + row['uuid'] + \")\" +\n",
    "              \" is too short. Description: \" + index)\n",
    "        continue\n",
    "\n",
    "        \n",
    "        #if None in keyphrases:\n",
    "        \n",
    "    \n",
    "tmp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23680847067\n"
     ]
    }
   ],
   "source": [
    "# Get the list of paths to articles files and their sizes\n",
    "paths = [(company, os.path.join(basedir, \"News_1\", \"\".join([company.upper(), \".json\"]))) for company in companies_list]\n",
    "paths = [path + (os.stat(path[1]).st_size,) for path in paths]\n",
    "# Sort by sizes\n",
    "paths = sorted(paths, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "full_size = 0\n",
    "for f_size in paths:\n",
    "    full_size = full_size + f_size[2]\n",
    "print(full_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonstreamer import JSONStreamer \n",
    "streamer = JSONStreamer()\n",
    "for index_f, path in enumerate(paths):\n",
    "    with open(path[1]) as file: \n",
    "        for line in file: \n",
    "            streamer.consume(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAMKA\n",
      "MAMKA\n",
      "MAMKA\n",
      "MAMKA\n",
      "MAMKA\n",
      "MAMKA\n",
      "MAMKA\n",
      "MAMKA\n",
      "MAMKA\n",
      "MAMKA\n",
      "MAMKA\n"
     ]
    }
   ],
   "source": [
    "import ijson.backends.yajl2_cffi as ijson\n",
    "def load_json(filename):\n",
    "    with open(filename, 'rb') as fd:\n",
    "        parser = ijson.parse(fd)\n",
    "        return parser\n",
    "def objects(self,file):\n",
    "    key = '-'\n",
    "    for prefix, event, value in ijson.parse(file):\n",
    "        if prefix == '' and event == 'map_key':  # found new object at the root\n",
    "            key = value  # mark the key value\n",
    "            builder = ObjectBuilder()\n",
    "        elif prefix.startswith(key):  # while at this key, build the object\n",
    "            builder.event(event, value)\n",
    "            if prefix == key+\".item\" and event == 'end_map':  # found the end of an object at the current key, yield\n",
    "                yield key, builder.value \n",
    "for index_f, path in enumerate(paths):\n",
    "    if index_f > 10:\n",
    "        break\n",
    "        \n",
    "    print('MAMKA') \n",
    "    templist = load_json(path[1])\n",
    "    with open(path[1], 'rb') as fd:\n",
    "        #for key, value in objects(fd):\n",
    "            #print(key, value)\n",
    "        objectss = ijson.items(fd, \"item\")\n",
    "        for item in objectss:\n",
    "            gg = item\n",
    "            \n",
    "    #print(templist.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9936"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies_df = tmp\n",
    "len(k_company_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9914"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_company_df = k_company_df.dropna()\n",
    "companies_list = k_company_df.index.get_values()\n",
    "len(k_company_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k_company_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f630f16e72cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'temp_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'temp_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mk_company_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'temp_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k_company_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mk_company_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'temp_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k_company_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'k_company_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Save for later use with keywords extraction and load\n",
    "if not os.path.exists(os.path.join(basedir, 'temp_data')):\n",
    "    os.makedirs(os.path.join(basedir, 'temp_data'))\n",
    "k_company_df.to_csv(os.path.join(basedir, 'temp_data', 'k_company_df.csv'), sep=\";\")\n",
    "\n",
    "k_company_df = pd.read_csv(os.path.join(basedir, 'temp_data', 'k_company_df.csv'), sep=\";\")\n",
    "k_company_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = pd.DataFrame()\n",
    "num_articles = 0\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index_f, file in enumerate(paths):\n",
    "    #if index_f > 0:\n",
    "    #    break\n",
    "    \n",
    "    print(file)\n",
    "    print(index_f)\n",
    "\n",
    "    try:\n",
    "        articles = pd.read_json(file[1])\n",
    "    except ValueError:\n",
    "        print(\"empty/faulty file\")\n",
    "        continue\n",
    "\n",
    "    if \"content\" not in articles.columns:\n",
    "        print(\"no content\")\n",
    "        continue\n",
    "\n",
    "    articles.loc[:, \"company\"] = file[0]\n",
    "    articles.loc[:, \"uuid\"] = k_company_df.loc[file[0].lower(), \"uuid\"]\n",
    "    articles.loc[:, \"description\"] = k_company_df.loc[file[0].lower(), \"description\"]\n",
    "\n",
    "    articles.dropna(subset=[\"content\"], inplace=True)\n",
    "\n",
    "    texts = [regex.sub('', i).lower() for i in articles.loc[:, \"content\"]]\n",
    "    keyphrases = ast.literal_eval(k_company_df.loc[file[0].lower(), \"kp_topicrank\"])\n",
    "    num_articles = num_articles + len(texts)\n",
    "\n",
    "    # Check whether the any of the keywords exist in the text\n",
    "    bi = [1 \n",
    "             if any([keyphrase in text \n",
    "                     for (keyphrase, prob) in (keyphrases)]) and 250 < len(str.split(text)) < 500 \n",
    "             else 0\n",
    "            for text in texts]\n",
    "\n",
    "    articles.loc[:, \"kp_topicrank_bi\"] = bi\n",
    "    articles.dropna(subset=[\"content\"], inplace=True)\n",
    "\n",
    "    all_articles = all_articles.append(articles, ignore_index=True, sort=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       NaN\n",
       "1       NaN\n",
       "2       NaN\n",
       "3       NaN\n",
       "4       NaN\n",
       "5       NaN\n",
       "6       NaN\n",
       "7       NaN\n",
       "8       NaN\n",
       "9       NaN\n",
       "10      NaN\n",
       "11      NaN\n",
       "12      NaN\n",
       "13      NaN\n",
       "14      NaN\n",
       "15      NaN\n",
       "16      NaN\n",
       "17      NaN\n",
       "18      NaN\n",
       "19      NaN\n",
       "20      NaN\n",
       "21      NaN\n",
       "22      NaN\n",
       "23      NaN\n",
       "24      NaN\n",
       "25      NaN\n",
       "26      NaN\n",
       "27      NaN\n",
       "28      NaN\n",
       "29      NaN\n",
       "       ... \n",
       "5662    0.0\n",
       "5663    0.0\n",
       "5664    0.0\n",
       "5665    1.0\n",
       "5666    1.0\n",
       "5667    0.0\n",
       "5668    0.0\n",
       "5669    0.0\n",
       "5670    0.0\n",
       "5671    0.0\n",
       "5672    0.0\n",
       "5673    0.0\n",
       "5674    0.0\n",
       "5675    0.0\n",
       "5676    0.0\n",
       "5677    0.0\n",
       "5678    0.0\n",
       "5679    0.0\n",
       "5680    0.0\n",
       "5681    0.0\n",
       "5682    0.0\n",
       "5683    0.0\n",
       "5684    0.0\n",
       "5685    0.0\n",
       "5686    0.0\n",
       "5687    0.0\n",
       "5688    0.0\n",
       "5689    1.0\n",
       "5690    1.0\n",
       "5691    0.0\n",
       "Name: kp_topicrank_bi, Length: 5692, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[\"kp_topicrank_bi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% of articles were are containing corresponding keyphrases extracted from the descriptions of the companies\n"
     ]
    }
   ],
   "source": [
    "print(str(len(all_articles/num_articles)) + \"% of articles were are containing corresponding keyphrases extracted from the descriptions of the companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "if not os.path.exists(os.path.join(basedir, 'temp_data')):\n",
    "    os.makedirs(os.path.join(basedir, 'temp_data'))\n",
    "all_articles.to_csv(os.path.join(basedir, 'temp_data', 'all_articles.csv'), sep=\";\")\n",
    "all_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABRA', 'ACRONIS', 'ACTON', 'AIRSTONE', 'ALBERT', 'ALEA', 'ALIANZA', 'AMARA', 'ANDO', 'ARKIN', 'ARTSPACE', 'ASANA', 'ASOKA', 'ASTRID', 'ATHOS', 'AVA', 'AYR', 'BACKDOOR', 'BANNERMAN', 'BARKLY', 'BASTILLE', 'BBB', 'BEBO', 'BENTO', 'BIA', 'BINDO', 'BIOME', 'BIRDBOX', 'BITTORRENT', 'BIX', 'BIZEN', 'BOARDVOTE', 'BONOBOS', 'BOTTLENOSE', 'BOWERY', 'BRAINTREE', 'BRIT', 'BUDDYTV', 'CAIS', 'CALERA', 'CALVIN', 'CARTESIAN', 'CASPER', 'CASSATT', 'CENTRO', 'CERBERUS', 'CHAI', 'CLARK', 'CLEO', 'CLICKABLE', 'CLOE', 'COTOPAXI', 'CRONO', 'CROSSFADER', 'DAILYMOTION', 'DELOS', 'DIGBY', 'DIY', 'DOO', 'DOPPELGANGER', 'ECO', 'ELASTICA', 'ELEMENTUM', 'ELLIE', 'ELÉ˜', 'EMMA', 'ENDORPHIN', 'EPIPHYTE', 'EPS', 'ESS', 'EVERLY', 'EXCALIBUR', 'EXOGENESIS', 'EYESPOT', 'FAB', 'FACEBOOK', 'FANBASE', 'FARADAY', 'FIDELIS', 'FLASHPOINT', 'FLAVOUR', 'FLYER', 'FLYNN', 'FONDU', 'FORMSPRING', 'FRONTO', 'FRS', 'FULHAM', 'FUZE', 'FY', 'GAMETIME', 'GAMEZONE', 'GAMGEE', 'GELI', 'GENI', 'GOLDSTAR', 'GOLGI', 'GORGIAS', 'GRO', 'HALLUX', 'HARA', 'HEDVIG', 'HELIOS', 'HOMIE', 'HOUDINI', 'IGG', 'IGOR', 'IMVU', 'INTELLIVISION', 'JANA', 'JETHRO', 'JOLIE', 'JOOST', 'JOSEPHINE', 'JUNE', 'KAHUNA', 'KASH', 'KAT', 'KATO', 'KENNA', 'KIKO', 'KIVA', 'KLARA', 'KNO', 'KRAFTWERK', 'KRAK', 'KUDO', 'KUNA', 'KWIK', 'LALA', 'LEET', 'LEO', 'LEVERTON', 'LIA', 'LIFECYCLE', 'LIFESIZE', 'LIMA', 'LOLA', 'LOUP', 'LUA', 'LUKA', 'LULLY', 'LUMA', 'LUMI', 'LUMINOSO', 'LUX', 'MARIANA', 'MARKHOR', 'MASALA', 'MATCHA', 'MAZ', 'MEDIO', 'MELBA', 'METADATA', 'MIKA', 'MILA', 'MILO', 'MIRA', 'MISO', 'MITRO', 'MODELO', 'MOLI', 'MOSHI', 'MOWGLI', 'MPV', 'MYCROFT', 'MYSPACE', 'NAV', 'NELLO', 'NETBOOKS', 'NETMINDER', 'NEWCO', 'NICO', 'NITRO', 'NOM', 'NORSE', 'NOVI', 'NULATO', 'OCTAVIAN', 'OCULUS', 'OFFLINE', 'OLLIE', 'ONO', 'OPER', 'ORCA', 'OSCAR', 'OSMO', 'OSSIA', 'OUTRO', 'OVALIS', 'PANA', 'PARSABLE', 'PELOTON', 'PERSEUS', 'PHENOM', 'PHILO', 'PIPEFISH', 'PIRC', 'PLAYDATE', 'PLAYLIST', 'PLEX', 'POPUP', 'PUTNEY', 'RAMEN', 'RC', 'RECURVE', 'REDDIT', 'REDOX', 'RIDLEY', 'RIVA', 'RIZE', 'RIZM', 'ROBBY', 'ROKU', 'RPO', 'SAMSARA', 'SANO', 'SEATERS', 'SEGOVIA', 'SHIPPO', 'SHOTO', 'SIGFIG', 'SIRI', 'SNOWDON', 'SOCI', 'SOCRATIC', 'SOLEY', 'SPOCK', 'STRATOS', 'STUMBLEUPON', 'SUNFIRE', 'SUPERFLY', 'SZL', 'TALKSPACE', 'TECHNORATI', 'TELEPORT', 'TENJIN', 'TRAVIS', 'TREEHOUSE', 'TREVENA', 'TUUL', 'TWOFISH', 'TYCHE', 'UBER', 'UMAMI', 'UNDERSTORY', 'UNU', 'UNUM', 'VAYU', 'VENGA', 'VENIAM', 'VERA', 'VERT', 'VIDA', 'VIDI', 'VIGO', 'VINCI', 'VING', 'VIV', 'VUE', 'WALDO', 'WASABI', 'WHIPTAIL', 'WIKIA', 'WILDCARD', 'WINNIE', 'WUNDER', 'XING', 'YANTRA', 'YELLO', 'YORN', 'YOSHI', 'YOUTUBE', 'YUME', 'ZAZA', 'ZEEBO', 'ZEO', 'ZOLA', 'ZOOSK', 'ZORA', 'ZUKI']\n"
     ]
    }
   ],
   "source": [
    "print(companies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76920"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNPARSED</th>\n",
       "      <th>autor</th>\n",
       "      <th>byline</th>\n",
       "      <th>contact</th>\n",
       "      <th>content</th>\n",
       "      <th>dateline</th>\n",
       "      <th>distribution</th>\n",
       "      <th>error</th>\n",
       "      <th>error_code</th>\n",
       "      <th>grafic</th>\n",
       "      <th>...</th>\n",
       "      <th>pub_type</th>\n",
       "      <th>rubrik</th>\n",
       "      <th>section</th>\n",
       "      <th>series</th>\n",
       "      <th>source</th>\n",
       "      <th>company</th>\n",
       "      <th>uuid</th>\n",
       "      <th>description</th>\n",
       "      <th>country</th>\n",
       "      <th>correction_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4395</th>\n",
       "      <td>AFX - Asia\\n\\n                     November 2,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AFX - Asia\\n\\n                     November 2,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>musx</td>\n",
       "      <td>9d2a0961-a4dc-ae6e-886b-64246ca3f4ab</td>\n",
       "      <td>Whenever you first meet someone, there is the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>AFX - Asia\\n\\n                      August 16,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AFX - Asia\\n\\n                      August 16,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>musx</td>\n",
       "      <td>9d2a0961-a4dc-ae6e-886b-64246ca3f4ab</td>\n",
       "      <td>Whenever you first meet someone, there is the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>Manila Times\\n\\n                             J...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manila Times\\n\\n                             J...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>musx</td>\n",
       "      <td>9d2a0961-a4dc-ae6e-886b-64246ca3f4ab</td>\n",
       "      <td>Whenever you first meet someone, there is the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>AFX - Asia\\n\\n                       May 24, 2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AFX - Asia\\n\\n                       May 24, 2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>musx</td>\n",
       "      <td>9d2a0961-a4dc-ae6e-886b-64246ca3f4ab</td>\n",
       "      <td>Whenever you first meet someone, there is the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>AFX - Asia\\n\\n                       April 22,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AFX - Asia\\n\\n                       April 22,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>musx</td>\n",
       "      <td>9d2a0961-a4dc-ae6e-886b-64246ca3f4ab</td>\n",
       "      <td>Whenever you first meet someone, there is the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               UNPARSED autor byline contact  \\\n",
       "4395  AFX - Asia\\n\\n                     November 2,...   NaN    NaN     NaN   \n",
       "4396  AFX - Asia\\n\\n                      August 16,...   NaN    NaN     NaN   \n",
       "4397  Manila Times\\n\\n                             J...   NaN    NaN     NaN   \n",
       "4398  AFX - Asia\\n\\n                       May 24, 2...   NaN    NaN     NaN   \n",
       "4399  AFX - Asia\\n\\n                       April 22,...   NaN    NaN     NaN   \n",
       "\n",
       "                                                content dateline distribution  \\\n",
       "4395  AFX - Asia\\n\\n                     November 2,...      NaN          NaN   \n",
       "4396  AFX - Asia\\n\\n                      August 16,...      NaN          NaN   \n",
       "4397  Manila Times\\n\\n                             J...      NaN          NaN   \n",
       "4398  AFX - Asia\\n\\n                       May 24, 2...      NaN          NaN   \n",
       "4399  AFX - Asia\\n\\n                       April 22,...      NaN          NaN   \n",
       "\n",
       "     error  error_code grafic       ...       pub_type rubrik section series  \\\n",
       "4395   NaN         NaN    NaN       ...            NaN    NaN     NaN    NaN   \n",
       "4396   NaN         NaN    NaN       ...            NaN    NaN     NaN    NaN   \n",
       "4397   NaN         NaN    NaN       ...            NaN    NaN     NaN    NaN   \n",
       "4398   NaN         NaN    NaN       ...            NaN    NaN     NaN    NaN   \n",
       "4399   NaN         NaN    NaN       ...            NaN    NaN     NaN    NaN   \n",
       "\n",
       "     source company                                  uuid  \\\n",
       "4395    NaN    musx  9d2a0961-a4dc-ae6e-886b-64246ca3f4ab   \n",
       "4396    NaN    musx  9d2a0961-a4dc-ae6e-886b-64246ca3f4ab   \n",
       "4397    NaN    musx  9d2a0961-a4dc-ae6e-886b-64246ca3f4ab   \n",
       "4398    NaN    musx  9d2a0961-a4dc-ae6e-886b-64246ca3f4ab   \n",
       "4399    NaN    musx  9d2a0961-a4dc-ae6e-886b-64246ca3f4ab   \n",
       "\n",
       "                                            description country  \\\n",
       "4395  Whenever you first meet someone, there is the ...     NaN   \n",
       "4396  Whenever you first meet someone, there is the ...     NaN   \n",
       "4397  Whenever you first meet someone, there is the ...     NaN   \n",
       "4398  Whenever you first meet someone, there is the ...     NaN   \n",
       "4399  Whenever you first meet someone, there is the ...     NaN   \n",
       "\n",
       "     correction_date  \n",
       "4395             NaN  \n",
       "4396             NaN  \n",
       "4397             NaN  \n",
       "4398             NaN  \n",
       "4399             NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
