{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/artemiy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/artemiy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/artemiy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Install uncommon packages (will supply requements.txt later)\n",
    "#!pip install git+https://github.com/boudinfl/pke.git\n",
    "#!pip install wordninja\n",
    "#!pip install pyenchant\n",
    "\n",
    "# For macos:\n",
    "#brew install cmake\n",
    "#git clone git@github.com:lloyd/yajl.git\n",
    "#cd yajl\n",
    "#./configure && make install\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import wordninja\n",
    "from os import listdir\n",
    "import re\n",
    "import sys\n",
    "import ast\n",
    "import pke\n",
    "import enchant\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import requests\n",
    "import pickle\n",
    "import datetime\n",
    "import ijson.backends.yajl2_cffi as ijson\n",
    "\n",
    "\n",
    "#import pycorpora\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "d = enchant.Dict(\"en_US\") # ENGLISH Dictionary\n",
    "\n",
    "\n",
    "# CHANGE THE PATH TO A DATASET HERE\n",
    "basedir = os.path.join(os.sep, \"Users\", \"artemiy\", \"Disk-O\", \"tyomkolton@mail.ru-mailru\", \"ML\")\n",
    "path_to_embedrank_repo = os.path.join(\"keyphrase\", \"ai-research-keyphrase-extraction\") # relative (without \"../\") or absolute path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded descriptions\n",
      "loaded org infos\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(basedir, \"CB_export\", \"cb_organization_descriptions.csv\"), index_col=\"uuid\")\n",
    "#data.iloc[:,1].to_csv('CB_Export_17_08_07/descr.txt')\n",
    "print(\"loaded descriptions\")\n",
    "data1 = pd.read_csv(os.path.join(basedir, \"CB_export\", \"cb_organizations.csv\"), low_memory=False, index_col=\"uuid\")\n",
    "print(\"loaded org infos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged\n"
     ]
    }
   ],
   "source": [
    "# Merge 2 csv files\n",
    "# we can time to see which is more effective\n",
    "#result = pd.concat([data, data1], axis=1, sort=False)\n",
    "result1 = pd.merge(data, data1, how='inner', left_index=True, right_index=True)\n",
    "print(\"merged\")\n",
    "# Delete all companies that do not have any descriptions\n",
    "result1 = result1.dropna(subset=[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(((result.shape[0] - result1.shape[0])/result.shape[0]*100), \"% of companies do not have any description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>company_name</th>\n",
       "      <th>primary_role</th>\n",
       "      <th>permalink</th>\n",
       "      <th>domain</th>\n",
       "      <th>homepage_url</th>\n",
       "      <th>country_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>phone</th>\n",
       "      <th>facebook_url</th>\n",
       "      <th>linkedin_url</th>\n",
       "      <th>cb_url</th>\n",
       "      <th>logo_url</th>\n",
       "      <th>profile_image_url</th>\n",
       "      <th>twitter_url</th>\n",
       "      <th>alias</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uuid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f5bb580f-d655-cf3f-9ade-eca5b3f2719f</th>\n",
       "      <td>Android, Apple - iOS, Blackberry, Windows Phon...</td>\n",
       "      <td>VilarikA</td>\n",
       "      <td>company</td>\n",
       "      <td>/organization/vilarika</td>\n",
       "      <td>vilarika.com.br</td>\n",
       "      <td>http://vilarika.com.br/</td>\n",
       "      <td>BRA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>Belo Horizonte</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.crunchbase.com/organization/vilarika</td>\n",
       "      <td>https://www.crunchbase.com/organization/vilari...</td>\n",
       "      <td>http://public.crunchbase.com/t_api_images/v141...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-12-11 06:46:05</td>\n",
       "      <td>2016-09-07 00:03:51.67913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000aa4-ba42-9b68-a9c3-040c9f3bf9b9</th>\n",
       "      <td>Formel D GmbH is a automotive manufacturer and...</td>\n",
       "      <td>Formel D GmbH</td>\n",
       "      <td>company</td>\n",
       "      <td>/organization/formel-d-gmbh</td>\n",
       "      <td>formeld.com</td>\n",
       "      <td>http://www.formeld.com</td>\n",
       "      <td>DEU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DEU - Other</td>\n",
       "      <td>Troisdorf</td>\n",
       "      <td>...</td>\n",
       "      <td>+49 2241 9960</td>\n",
       "      <td>https://www.facebook.com/formeld</td>\n",
       "      <td>https://www.linkedin.com/company/formel-d-group</td>\n",
       "      <td>https://www.crunchbase.com/organization/formel...</td>\n",
       "      <td>https://www.crunchbase.com/organization/formel...</td>\n",
       "      <td>http://public.crunchbase.com/t_api_images/v148...</td>\n",
       "      <td>https://www.twitter.com/formeld_es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-06-01 06:58:37.692725</td>\n",
       "      <td>2017-07-18 07:22:19.15864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5bd38d1-7719-2935-fb6c-defae39f5b93</th>\n",
       "      <td>[iNFoGooL](http://infogool.com) - The Informat...</td>\n",
       "      <td>infogool</td>\n",
       "      <td>company</td>\n",
       "      <td>/organization/infogool</td>\n",
       "      <td>infogool.com</td>\n",
       "      <td>http://infogool.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8802393009</td>\n",
       "      <td>http://www.facebook.com/infogool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.crunchbase.com/organization/infogool</td>\n",
       "      <td>https://www.crunchbase.com/organization/infogo...</td>\n",
       "      <td>http://public.crunchbase.com/t_api_images/v139...</td>\n",
       "      <td>https://www.twitter.com/infogool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-04-12 00:10:12</td>\n",
       "      <td>2016-09-08 22:02:05.585875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773adc18-132a-937d-8841-4f833e64dd56</th>\n",
       "      <td>The Jinfeng Gold Mine is an combined open pit ...</td>\n",
       "      <td>Jinfeng Mine</td>\n",
       "      <td>company</td>\n",
       "      <td>/organization/jinfeng-mine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.crunchbase.com/organization/jinfen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-07 06:43:06.955155</td>\n",
       "      <td>2016-12-07 06:45:09.14727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5bdd48a-a09f-2f4c-bd71-4c4207c7731b</th>\n",
       "      <td>Peardoc offers online tools to convert HTML to...</td>\n",
       "      <td>Peardoc Solutions</td>\n",
       "      <td>company</td>\n",
       "      <td>/organization/peardoc-solutions</td>\n",
       "      <td>peardoc.com</td>\n",
       "      <td>http://www.peardoc.com</td>\n",
       "      <td>IND</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.crunchbase.com/organization/peardo...</td>\n",
       "      <td>https://www.crunchbase.com/organization/peardo...</td>\n",
       "      <td>http://public.crunchbase.com/t_api_images/v140...</td>\n",
       "      <td>https://www.twitter.com/pear_doc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-07-17 10:06:03</td>\n",
       "      <td>2016-03-08 02:56:01.230586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            description  \\\n",
       "uuid                                                                                      \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f  Android, Apple - iOS, Blackberry, Windows Phon...   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  Formel D GmbH is a automotive manufacturer and...   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93  [iNFoGooL](http://infogool.com) - The Informat...   \n",
       "773adc18-132a-937d-8841-4f833e64dd56  The Jinfeng Gold Mine is an combined open pit ...   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  Peardoc offers online tools to convert HTML to...   \n",
       "\n",
       "                                           company_name primary_role  \\\n",
       "uuid                                                                   \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f           VilarikA      company   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9      Formel D GmbH      company   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93           infogool      company   \n",
       "773adc18-132a-937d-8841-4f833e64dd56       Jinfeng Mine      company   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  Peardoc Solutions      company   \n",
       "\n",
       "                                                            permalink  \\\n",
       "uuid                                                                    \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f           /organization/vilarika   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9      /organization/formel-d-gmbh   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93           /organization/infogool   \n",
       "773adc18-132a-937d-8841-4f833e64dd56       /organization/jinfeng-mine   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  /organization/peardoc-solutions   \n",
       "\n",
       "                                               domain  \\\n",
       "uuid                                                    \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f  vilarika.com.br   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9      formeld.com   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93     infogool.com   \n",
       "773adc18-132a-937d-8841-4f833e64dd56              NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b      peardoc.com   \n",
       "\n",
       "                                                 homepage_url country_code  \\\n",
       "uuid                                                                         \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f  http://vilarika.com.br/          BRA   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9   http://www.formeld.com          DEU   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93      http://infogool.com          NaN   \n",
       "773adc18-132a-937d-8841-4f833e64dd56                      NaN          CHN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b   http://www.peardoc.com          IND   \n",
       "\n",
       "                                     state_code          region  \\\n",
       "uuid                                                              \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f        NaN  Rio de Janeiro   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9        NaN     DEU - Other   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93        NaN             NaN   \n",
       "773adc18-132a-937d-8841-4f833e64dd56        NaN             NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b        NaN         Chennai   \n",
       "\n",
       "                                                city  \\\n",
       "uuid                                                   \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f  Belo Horizonte   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9       Troisdorf   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93             NaN   \n",
       "773adc18-132a-937d-8841-4f833e64dd56             NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b         Chennai   \n",
       "\n",
       "                                                 ...              \\\n",
       "uuid                                             ...               \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f             ...               \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9             ...               \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93             ...               \n",
       "773adc18-132a-937d-8841-4f833e64dd56             ...               \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b             ...               \n",
       "\n",
       "                                              phone  \\\n",
       "uuid                                                  \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f            NaN   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  +49 2241 9960   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93     8802393009   \n",
       "773adc18-132a-937d-8841-4f833e64dd56            NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b            NaN   \n",
       "\n",
       "                                                          facebook_url  \\\n",
       "uuid                                                                     \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f                               NaN   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  https://www.facebook.com/formeld   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93  http://www.facebook.com/infogool   \n",
       "773adc18-132a-937d-8841-4f833e64dd56                               NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b                               NaN   \n",
       "\n",
       "                                                                         linkedin_url  \\\n",
       "uuid                                                                                    \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f                                              NaN   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  https://www.linkedin.com/company/formel-d-group   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93                                              NaN   \n",
       "773adc18-132a-937d-8841-4f833e64dd56                                              NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b                                              NaN   \n",
       "\n",
       "                                                                                 cb_url  \\\n",
       "uuid                                                                                      \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f   https://www.crunchbase.com/organization/vilarika   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  https://www.crunchbase.com/organization/formel...   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93   https://www.crunchbase.com/organization/infogool   \n",
       "773adc18-132a-937d-8841-4f833e64dd56  https://www.crunchbase.com/organization/jinfen...   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  https://www.crunchbase.com/organization/peardo...   \n",
       "\n",
       "                                                                               logo_url  \\\n",
       "uuid                                                                                      \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f  https://www.crunchbase.com/organization/vilari...   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  https://www.crunchbase.com/organization/formel...   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93  https://www.crunchbase.com/organization/infogo...   \n",
       "773adc18-132a-937d-8841-4f833e64dd56                                                NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  https://www.crunchbase.com/organization/peardo...   \n",
       "\n",
       "                                                                      profile_image_url  \\\n",
       "uuid                                                                                      \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f  http://public.crunchbase.com/t_api_images/v141...   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  http://public.crunchbase.com/t_api_images/v148...   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93  http://public.crunchbase.com/t_api_images/v139...   \n",
       "773adc18-132a-937d-8841-4f833e64dd56                                                NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  http://public.crunchbase.com/t_api_images/v140...   \n",
       "\n",
       "                                                             twitter_url  \\\n",
       "uuid                                                                       \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f                                 NaN   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9  https://www.twitter.com/formeld_es   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93    https://www.twitter.com/infogool   \n",
       "773adc18-132a-937d-8841-4f833e64dd56                                 NaN   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b    https://www.twitter.com/pear_doc   \n",
       "\n",
       "                                      alias                  created_at  \\\n",
       "uuid                                                                      \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f    NaN         2014-12-11 06:46:05   \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9    NaN  2016-06-01 06:58:37.692725   \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93    NaN         2014-04-12 00:10:12   \n",
       "773adc18-132a-937d-8841-4f833e64dd56    NaN  2016-12-07 06:43:06.955155   \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b    NaN         2014-07-17 10:06:03   \n",
       "\n",
       "                                                      updated_at  \n",
       "uuid                                                              \n",
       "f5bb580f-d655-cf3f-9ade-eca5b3f2719f   2016-09-07 00:03:51.67913  \n",
       "00000aa4-ba42-9b68-a9c3-040c9f3bf9b9   2017-07-18 07:22:19.15864  \n",
       "f5bd38d1-7719-2935-fb6c-defae39f5b93  2016-09-08 22:02:05.585875  \n",
       "773adc18-132a-937d-8841-4f833e64dd56   2016-12-07 06:45:09.14727  \n",
       "f5bdd48a-a09f-2f4c-bd71-4c4207c7731b  2016-03-08 02:56:01.230586  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353146"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constuct a list of companies that have descriptions\n",
    "#d_companies_list = list(result1.loc[:, \"company_name\"])\n",
    "#d_companies_list = [company.lower() for company in d_companies_list]\n",
    "#len(d_companies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20249\n"
     ]
    }
   ],
   "source": [
    "# Construct a list of companies from dataset folder\n",
    "f_companies_list = os.listdir(os.path.join(basedir, \"News_1\"))\n",
    "# Get rid of \".json\" extension and apply uppercase\n",
    "f_companies_list = [re.sub(\".json\", \"\", company) for company in f_companies_list]\n",
    "print(len(f_companies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12609\n",
      "11385\n"
     ]
    }
   ],
   "source": [
    "# Get rid of companies with names containing anything else than latin letters (as well as containing \"_\" separating multiple words in names)\n",
    "f_companies_list = [company.lower() for company in f_companies_list if all(letter in string.ascii_letters for letter in company)]\n",
    "print(len(f_companies_list))\n",
    "\n",
    "# Get rid of companies whose full names consist of one single word from english dictionary\n",
    "f_companies_list = [company for company in f_companies_list if not d.check(company.lower())]\n",
    "print(len(f_companies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accend', 'acton', 'aerospike', 'afterschool', 'ais', 'alation', 'alector', 'alignable', 'amberjack', 'amicus', 'ananas', 'ansible', 'anyroad', 'aptera', 'aruspex', 'asana', 'attask', 'auris', 'authorly', 'aviso', 'ayr', 'backchat', 'backdoor', 'backtrace', 'bannerman', 'bastille', 'beme', 'bento', 'bevvy', 'biome', 'biscotti', 'bittorrent', 'blesh', 'bonafide', 'bonobos', 'bookbag', 'boombox', 'booyah', 'bottlenose', 'bowery', 'boxfish', 'brit', 'browster', 'carbonite', 'cardio', 'cartesian', 'cartogram', 'celly', 'centro', 'chargeback', 'chockstone', 'chronicity', 'clickable', 'cloze', 'containership', 'contentful', 'convo', 'copromote', 'courseload', 'courseloads', 'cringle', 'crossfader', 'cuculus', 'dekko', 'dextro', 'digby', 'doo', 'doozie', 'doppelganger', 'driverside', 'dropoff', 'dropship', 'duetto', 'eco', 'elastica', 'elsen', 'emma', 'endorphin', 'endplay', 'enow', 'entrada', 'epiphyte', 'eponym', 'eps', 'ess', 'everly', 'evolv', 'evolver', 'exogenesis', 'eyespot', 'fab', 'facebook', 'fanbase', 'faraday', 'fastly', 'faves', 'figma', 'figo', 'firebase', 'firethorn', 'fitt', 'fixt', 'flashpoint', 'flavour', 'flite', 'fluential', 'fluther', 'flyer', 'fogger', 'fondu', 'foodist', 'fora', 'forerun', 'fulham', 'fundation', 'fuze', 'fy', 'getable', 'gett', 'globality', 'glyde', 'goby', 'goji', 'goot', 'grabble', 'graphicly', 'greenling', 'gridpoint', 'hability', 'hallux', 'hangtime', 'happify', 'hashable', 'headnote', 'headspace', 'higgle', 'highspot', 'homie', 'hotlink', 'hotlist', 'housecall', 'huckle', 'hux', 'hyr', 'igg', 'ilike', 'inlist', 'innit', 'insense', 'inturn', 'invision', 'iterable', 'jobble', 'jobster', 'jumpcut', 'jumpshot', 'kaboodle', 'kahuna', 'kapow', 'kapta', 'kast', 'kat', 'keas', 'kensho', 'kickboard', 'kickup', 'kiva', 'knod', 'kudo', 'kuna', 'kyte', 'lanx', 'larky', 'leet', 'legalist', 'levo', 'lexigram', 'lifecycle', 'lifesize', 'lifestreams', 'lightswitch', 'lima', 'lookit', 'loup', 'loupe', 'loveseat', 'luma', 'lux', 'lyft', 'mahalo', 'makara', 'markhor', 'masala', 'mashable', 'matcha', 'matchpoint', 'mately', 'mediant', 'medio', 'meetup', 'melba', 'metadata', 'milo', 'miso', 'mobee', 'mochila', 'mog', 'moki', 'moli', 'mozy', 'mux', 'myspace', 'nav', 'netbooks', 'netminder', 'nextdoor', 'nitch', 'nitro', 'noesis', 'noke', 'nom', 'nota', 'nucleonics', 'oculus', 'offline', 'ogin', 'ollie', 'ombu', 'onefold', 'onlive', 'ono', 'oratio', 'orca', 'ossia', 'ouroboros', 'outro', 'overdog', 'overwatch', 'parsable', 'payscale', 'peloton', 'perq', 'petrichor', 'phenom', 'phil', 'phononic', 'pickie', 'pieris', 'pinscreen', 'pipefish', 'playdate', 'playlist', 'plex', 'pley', 'pluot', 'pomello', 'popin', 'popup', 'poshly', 'powerset', 'poynt', 'precog', 'pureplay', 'qualia', 'quartzy', 'quora', 'ramen', 'rebit', 'recurve', 'redd', 'redfin', 'redox', 'redux', 'remerge', 'repp', 'revaluate', 'revolv', 'ribbit', 'ridley', 'rize', 'rowl', 'rythm', 'saleswise', 'samsara', 'screenie', 'seaters', 'seeme', 'senet', 'sensoria', 'sente', 'serica', 'serverless', 'shapeup', 'shippo', 'shoutout', 'sinch', 'skift', 'skout', 'skully', 'smore', 'smyte', 'snapt', 'sofi', 'soko', 'solum', 'sonation', 'sonder', 'speek', 'splurgy', 'spoonflower', 'stickybeak', 'stormwind', 'stringify', 'strix', 'superfly', 'superphone', 'supersecret', 'talkable', 'tastebud', 'tastemaker', 'technorati', 'telematic', 'teleport', 'thanx', 'thru', 'thryve', 'tidemark', 'tidepool', 'timeful', 'tock', 'toro', 'tred', 'treehouse', 'trippy', 'trover', 'twelvefold', 'tyche', 'tympany', 'uber', 'umami', 'understory', 'unmute', 'upto', 'uru', 'vayu', 'vera', 'voxel', 'vroom', 'vue', 'waldo', 'wasabi', 'wavemaker', 'werk', 'wetpaint', 'whiptail', 'wildcard', 'wiselike', 'wize', 'wunder', 'wut', 'wynk', 'xing', 'yantra', 'yarly', 'yola', 'yowza', 'zafu']\n"
     ]
    }
   ],
   "source": [
    "def word_exists(word, apiKey=apiKey, apiUrl='http://api.wordnik.com/v4'):\n",
    "\n",
    "    # Construct the API URL for a random-word query\n",
    "    api_url = \"{baseurl}/words.json/search/{word}\".format(\n",
    "        baseurl=apiUrl,\n",
    "        word=word\n",
    "    )\n",
    "\n",
    "    parameters = {\n",
    "        'api_key': apiKey,\n",
    "    }\n",
    "\n",
    "    # Perform the query and store the HTTP response object\n",
    "    response = requests.get(api_url, params=parameters)\n",
    "\n",
    "    # Convert the response content to a list\n",
    "    # NOTE: The content is initially returned as a byte string\n",
    "    word_object = json.loads(response.content)\n",
    "\n",
    "    # Get the number word's instances in the dictionary\n",
    "    if word_object:\n",
    "        if word_object['searchResults'][0]['count'] > 0:\n",
    "            result = True\n",
    "        else:\n",
    "            result = False\n",
    "    else:\n",
    "        result = False\n",
    "\n",
    "    # Return the word as a string\n",
    "    return result\n",
    "\n",
    "# Same as the previous step (looking for english words) but with Wordnik API\n",
    "apiUrl = 'http://api.wordnik.com/v4'\n",
    "apiKey = 'b0a856e4cbe5c4e3f110d0527b901b7064f22a1c09e153547'\n",
    "# Extract the list words from Wordnik that DO exist\n",
    "wordnik_companies_list = [company for company in f_companies_list if word_exists(company)]\n",
    "print(len(wordnik_companies_list))\n",
    "\n",
    "# These words DO exist in Wordnik dictionary\n",
    "print((wordnik_companies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use \n",
    "if not os.path.exists(os.path.join(basedir, 'temp_data')):\n",
    "    os.makedirs(os.path.join(basedir, 'temp_data'))\n",
    "with open('wordnik_companies_list.txt', 'wb') as fp:\n",
    "    pickle.dump(wordnik_companies_list, fp)\n",
    "with open ('wordnik_companies_list.txt', 'rb') as fp:\n",
    "    wordnik_companies_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11385\n",
      "11385\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Tries to separate a name string into multiple words and if succeeds gets rid of the company (a lot of false positives)\n",
    "# companies_list = [company for company in companies_list if len(wordninja.split(company.lower())) < 2]\n",
    "# print(len(companies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11018\n",
      "['aaptiv', 'abaris', 'abbeypost', 'abcmob', 'abeo', 'abililife', 'abilto', 'abine', 'abiobot', 'ablexis', 'ablio', 'abodo', 'abogen', 'abom', 'aboutlife', 'aboutme', 'aboutone', 'aboutusorg', 'abra', 'absci', 'absmaterials', 'abusix', 'abvitro', 'abyrx', 'academiaedu', 'academixdirect', 'acadiasoft', 'accelera', 'accelereach', 'accelergy', 'accelgolf', 'accellos', 'accelo', 'accelops', 'acceptd', 'accera', 'accertify', 'accesssportsmediacom', 'acclaimd', 'acclarent', 'accountnow', 'accredible', 'accreon', 'accuitis', 'acculitx', 'accumen', 'accuradio', 'accuvein', 'accuwater', 'aceable', 'acebotai', 'acertiv', 'achaogen', 'achieveit', 'acompli', 'acopio', 'acousticeye', 'acquaintable', 'acquia', 'acrinta', 'acrisure', 'acrobatiq', 'acronis', 'acsian', 'actacell', 'actifio', 'actionality', 'actionsprout', 'actionx', 'activaero', 'actived', 'activegrid', 'activehours', 'activepath', 'activeprotective', 'activerain', 'activiter', 'activityhero', 'actmd', 'actualmeds', 'actuatedmedical', 'actx', 'acufocus', 'acumatica', 'acumera', 'acupera', 'adadapted', 'adadyn', 'adallom', 'adapteva', 'adaptiveblue', 'adaptly', 'adaptv', 'adara', 'adavium', 'adayana', 'adbrite', 'adcade', 'adchemy', 'addapp']\n"
     ]
    }
   ],
   "source": [
    "f_companies_list = [e for e in f_companies_list if e not in set(wordnik_companies_list)]\n",
    "print(len(f_companies_list))\n",
    "print(f_companies_list[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345890"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data frame of companies descriptions to be used in final data frame with articles\n",
    "d_companies_df = result1\n",
    "d_companies_df.loc[:,'company_name'] = d_companies_df.loc[:,'company_name'].str.lower()\n",
    "#d_companies_df = d_companies_df[d_companies_df['company_name'].str.lower().isin(d_companies_list)]\n",
    "#d_companies_df = d_companies_df[d_companies_df['company_name'].isin(d_companies_list)]\n",
    "d_companies_df = d_companies_df.reset_index().set_index(keys='company_name')#, verify_integrity=True)\n",
    "\n",
    "\n",
    "\n",
    "# Delete the companies which have the same name\n",
    "d_companies_df = d_companies_df[~d_companies_df.index.duplicated(keep=False)]\n",
    "\n",
    "\n",
    "d_companies_df = d_companies_df[['uuid', 'description']]\n",
    "len(d_companies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9658\n"
     ]
    }
   ],
   "source": [
    "# Delete companies outside or dataset of articles\n",
    "companies_df = d_companies_df[d_companies_df.index.isin(f_companies_list)]\n",
    "companies_list = companies_df.index.values\n",
    "print(len(companies_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use with keywords extraction\n",
    "if not os.path.exists(os.path.join(basedir, 'temp_data')):\n",
    "    os.makedirs(os.path.join(basedir, 'temp_data'))\n",
    "with open('companies_df.csv', 'wb') as fp:\n",
    "    pickle.dump(companies_df, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9658\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nerites</th>\n",
       "      <td>f5c05e57-d563-db35-8210-6f26d6a250d2</td>\n",
       "      <td>At Nerites, they combine world class technolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biovascular</th>\n",
       "      <td>7539d769-643c-6c26-5339-ec52bd5b81c2</td>\n",
       "      <td>BioVascular is a clinical stage privately held...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transbiotec</th>\n",
       "      <td>f5e29c35-a53e-635d-ce74-4013c11efec9</td>\n",
       "      <td>TransBiotec, a development stage company, focu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thrombovision</th>\n",
       "      <td>7758702e-6321-ee55-16d9-11a5dc9aaf34</td>\n",
       "      <td>ThromboVision is a biomedical company committe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fortressware</th>\n",
       "      <td>f1a53b32-a132-1764-9e2d-54e5324e94fc</td>\n",
       "      <td>Fortressware, Inc. provides solutions to secur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               uuid  \\\n",
       "company_name                                          \n",
       "nerites        f5c05e57-d563-db35-8210-6f26d6a250d2   \n",
       "biovascular    7539d769-643c-6c26-5339-ec52bd5b81c2   \n",
       "transbiotec    f5e29c35-a53e-635d-ce74-4013c11efec9   \n",
       "thrombovision  7758702e-6321-ee55-16d9-11a5dc9aaf34   \n",
       "fortressware   f1a53b32-a132-1764-9e2d-54e5324e94fc   \n",
       "\n",
       "                                                     description  \n",
       "company_name                                                      \n",
       "nerites        At Nerites, they combine world class technolog...  \n",
       "biovascular    BioVascular is a clinical stage privately held...  \n",
       "transbiotec    TransBiotec, a development stage company, focu...  \n",
       "thrombovision  ThromboVision is a biomedical company committe...  \n",
       "fortressware   Fortressware, Inc. provides solutions to secur...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open ('companies_df.csv', 'rb') as fp:\n",
    "    companies_df = pickle.load(fp)\n",
    "companies_list = companies_df.index.values\n",
    "print(len(companies_df))\n",
    "companies_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TWO WAYS TO EXTRACT KEYWORDS ###\n",
    "\n",
    "# With PKE Package - TopicRank\n",
    "k_companies_df = companies_df\n",
    "\n",
    "# initialize keyphrase extraction model, here TopicRank\n",
    "import pke\n",
    "\n",
    "# With EmbedRank\n",
    "cwd = os.getcwd()\n",
    "try:\n",
    "    os.chdir(os.path.join(os.path.dirname(cwd), path_to_embedrank_repo))\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        os.chdir(path_to_embedrank_repo)\n",
    "    except FileNotFoundError:\n",
    "        os.chdir(cwd) \n",
    "import launch\n",
    "print(os.getcwd())\n",
    "embedding_distributor = launch.load_local_embedding_distributor('en')\n",
    "pos_tagger = launch.load_local_pos_tagger('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>description</th>\n",
       "      <th>kp_topicrank</th>\n",
       "      <th>kp_embedrank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nerites</th>\n",
       "      <td>f5c05e57-d563-db35-8210-6f26d6a250d2</td>\n",
       "      <td>At Nerites, they combine world class technolog...</td>\n",
       "      <td>[('market', 0.09826419107155268), ('technical ...</td>\n",
       "      <td>mda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biovascular</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[('classâ€\\x9d products', 0.09608849550420087)...</td>\n",
       "      <td>(['human clinical trials', 'hemodialysis acces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transbiotec</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[('systems', 0.09053148884251386), ('developme...</td>\n",
       "      <td>(['blood alcohol detection system', 'construct...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              uuid  \\\n",
       "company_name                                         \n",
       "nerites       f5c05e57-d563-db35-8210-6f26d6a250d2   \n",
       "biovascular                                    NaN   \n",
       "transbiotec                                    NaN   \n",
       "\n",
       "                                                    description  \\\n",
       "company_name                                                      \n",
       "nerites       At Nerites, they combine world class technolog...   \n",
       "biovascular                                                 NaN   \n",
       "transbiotec                                                 NaN   \n",
       "\n",
       "                                                   kp_topicrank  \\\n",
       "company_name                                                      \n",
       "nerites       [('market', 0.09826419107155268), ('technical ...   \n",
       "biovascular   [('classâ€\\x9d products', 0.09608849550420087)...   \n",
       "transbiotec   [('systems', 0.09053148884251386), ('developme...   \n",
       "\n",
       "                                                   kp_embedrank  \n",
       "company_name                                                     \n",
       "nerites                                                     mda  \n",
       "biovascular   (['human clinical trials', 'hemodialysis acces...  \n",
       "transbiotec   (['blood alcohol detection system', 'construct...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_companies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4e9dd90dfd07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# format (i.e. a simple string of text) and preprocessing is carried out using nltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pke/base.py\u001b[0m in \u001b[0;36mread_text\u001b[0;34m(self, input_text, stemmer)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \"\"\"\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_raw_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pke/base.py\u001b[0m in \u001b[0;36mread_raw_document\u001b[0;34m(self, stemmer, input_text)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# parse the document using the preprocessed text parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRawTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# loop through the parsed sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pke/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, input_text)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mtuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \"\"\"\n\u001b[1;32m     96\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \"\"\"\n\u001b[0;32m-> 1235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \"\"\"\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \"\"\"\n\u001b[1;32m   1313\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1288\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for index, row in companies_df.iterrows():\n",
    "    \n",
    "    \n",
    "    counter +=1\n",
    "    if counter >3:\n",
    "        break\n",
    "    \n",
    "    extractor = pke.unsupervised.TopicRank()\n",
    "    # load the content of the document, here document is expected to be in raw\n",
    "    # format (i.e. a simple string of text) and preprocessing is carried out using nltk\n",
    "    s = row[\"description\"]\n",
    "    extractor.read_text(s)\n",
    "\n",
    "    # keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
    "    # and adjectives\n",
    "    extractor.candidate_selection()\n",
    "    k_companies_df.loc[index, 'kp_embedrank'] = \"mda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9ce0a3b6bf98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# format (i.e. a simple string of text) and preprocessing is carried out using nltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pke/base.py\u001b[0m in \u001b[0;36mread_text\u001b[0;34m(self, input_text, stemmer)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \"\"\"\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_raw_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pke/base.py\u001b[0m in \u001b[0;36mread_raw_document\u001b[0;34m(self, stemmer, input_text)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# parse the document using the preprocessed text parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRawTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# loop through the parsed sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pke/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, input_text)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mtuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \"\"\"\n\u001b[1;32m     96\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \"\"\"\n\u001b[0;32m-> 1235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \"\"\"\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \"\"\"\n\u001b[1;32m   1313\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1288\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "    try:\n",
    "        # candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "        extractor.candidate_weighting()\n",
    "        # N-best selection, keyphrases contains the 10 highest scored candidates as\n",
    "        # (keyphrase, score) tuples\n",
    "        keyphrases = extractor.get_n_best(n=5, stemming=False)\n",
    "        keyphrases = sorted(keyphrases, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        k_companies_df.loc[index, 'kp_topicrank'] = str(keyphrases)\n",
    "    \n",
    "    except ValueError:\n",
    "        print(\"TOPIC_RANK: Description for company \" + index + \n",
    "              \" (\" + row['uuid'] + \")\" +\n",
    "              \" is too short. Description: \" + index)\n",
    "        continue\n",
    "    \n",
    "    kp1 = launch.extract_keyphrases(embedding_distributor, pos_tagger, row[\"description\"], 5, 'en') \n",
    "    if None in kp1:\n",
    "        print(\"EMBED_RANK: Description for company \" + index + \n",
    "                  \" (\" + row['uuid'] + \")\" +\n",
    "                  \" is too short. Description: \" + index)\n",
    "        continue    \n",
    "    k_companies_df.loc[index, 'kp_embedrank'] = str(kp1)\n",
    "    \n",
    "    #k_companies_df.dropna(subset=[\"kp_topicrank\"], inplace=True)\n",
    "    #k_companies_df.dropna(subset=[\"kp_embedrank\"], inplace=True)\n",
    "k_companies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entire dataframe with keyphrases\n",
    "if not os.path.exists(os.path.join(basedir, 'temp_data')):\n",
    "    os.makedirs(os.path.join(basedir, 'temp_data'))\n",
    "with open('k_companies_df.csv', 'wb') as fp:\n",
    "    pickle.dump(k_companies_df, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('k_companies_df.csv', 'rb') as fp:\n",
    "    k_companies_df = pickle.load(fp)\n",
    "k_companies_list = k_companies_df.index.values\n",
    "k_companies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23680847067\n"
     ]
    }
   ],
   "source": [
    "# Get the list of paths to articles files and their sizes\n",
    "paths = [(company, os.path.join(basedir, \"News_1\", \"\".join([company.upper(), \".json\"]))) for company in k_companies_list]\n",
    "paths = [path + (os.stat(path[1]).st_size,) for path in paths]\n",
    "# Sort by sizes\n",
    "paths = sorted(paths, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "full_size = 0\n",
    "for f_size in paths:\n",
    "    full_size = full_size + f_size[2]\n",
    "print(full_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from jsonstreamer import JSONStreamer \n",
    "streamer = JSONStreamer()\n",
    "for index_f, path in enumerate(paths):\n",
    "    with open(path[1]) as file: \n",
    "        for line in file: \n",
    "            streamer.consume(line)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def load_json(filename):\n",
    "    with open(filename, 'rb') as fd:\n",
    "        parser = ijson.parse(fd)\n",
    "        return parser\n",
    "def objects(self,file):\n",
    "    key = '-'\n",
    "    for prefix, event, value in ijson.parse(file):\n",
    "        if prefix == '' and event == 'map_key':  # found new object at the root\n",
    "            key = value  # mark the key value\n",
    "            builder = ObjectBuilder()\n",
    "        elif prefix.startswith(key):  # while at this key, build the object\n",
    "            builder.event(event, value)\n",
    "            if prefix == key+\".item\" and event == 'end_map':  # found the end of an object at the current key, yield\n",
    "                yield key, builder.value \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-23 08:07:07.554985\n",
      "yes\n",
      "['global', 'english', 'middle', 'east', 'and', 'north', 'africa', 'financial', 'network', 'december', '23', '2017', 'saturday', 'the', 'installed', 'base', 'of', 'fleet', 'management', 'systems', 'in', 'russia', 'cis', 'and', 'eastern', 'europe', 'will', 'reach', '9', '1', 'million', 'by', '2021', 'link', 'to', 'story', 'according', 'to', 'a', 'new', 'research', 'report', 'from', 'the', 'm2m', 'iot', 'analyst', 'firm', 'berg', 'insight', 'the', 'number', 'of', 'active', 'fleet', 'management', 'systems', 'deployed', 'in', 'commercial', 'vehicle', 'fleets', 'in', 'russia', 'cis', 'and', 'eastern', 'europe', 'was', '4', '8', 'million', 'in', 'q4', '2016', 'growing', 'at', 'a', 'compound', 'annual', 'growth', 'rate', 'cagr', 'of', '13', '5', 'percent', 'this', 'number', 'is', 'expected', 'to', 'reach', '9', '1', 'million', 'by', '2021', 'the', 'russian', 'market', 'alone', 'accounts', 'for', 'a', 'significant', 'share', 'of', 'the', 'region', 's', 'total', 'installed', 'base', 'and', 'is', 'forecasted', 'to', 'grow', 'from', '2', '1', 'million', 'active', 'units', 'at', 'the', 'end', 'of', '2016', 'to', '3', '5', 'million', 'units', 'by', '2021', 'the', 'top', '15', 'providers', 'of', 'fleet', 'management', 'solutions', 'for', 'commercial', 'vehicles', 'across', 'russia', 'the', 'rest', 'of', 'the', 'cis', 'and', 'eastern', 'europe', 'together', 'have', 'a', 'combined', 'installed', 'base', 'of', 'over', '2', '6', 'million', 'active', 'units', 'in', 'the', 'region', 'and', 'around', 'half', 'of', 'the', 'market', 'is', 'even', 'represented', 'by', 'the', 'top', '10', 'players', 'the', 'leading', 'fleet', 'management', 'solution', 'providers', 'in', 'terms', 'of', 'installed', 'base', 'in', 'the', 'cis', 'and', 'eastern', 'europe', 'include', 'diverse', 'players', 'from', 'a', 'number', 'of', 'countries', 'belarus', 'based', 'gurtam', 'is', 'the', 'leading', 'fleet', 'management', 'software', 'provider', 'having', 'surpassed', 'the', 'milestone', 'of', '500', '000', 'vehicles', 'under', 'management', 'in', 'the', 'region', 'says', 'rickard', 'andersson', 'senior', 'analyst', 'berg', 'insight', 'he', 'adds', 'that', 'gurtam', 'is', 'focused', 'on', 'software', 'providing', 'a', 'hardware', 'agnostic', 'tracking', 'platform', 'offering', 'compatibility', 'with', 'over', '1', '300', 'different', 'device', 'models', 'from', 'hundreds', 'of', 'third', 'party', 'hardware', 'manufacturers', 'arvento', 'mobile', 'systems', 'from', 'turkey', 'and', 'technokom', 'based', 'in', 'russia', 'are', 'the', 'first', 'and', 'second', 'runners', 'up', 'followed', 'by', 'turkish', 'mobiliz', 'and', 'the', 'russian', 'players', 'nis', 'scout', 'and', 'navigator', 'group', 'continues', 'mr', 'andersson', 'additional', 'top', '15', 'players', 'include', 'russia', 'based', 'omnicomm', 'which', 'has', 'around', '100', '000', 'active', 'fleet', 'management', 'units', 'as', 'well', 'as', 'infotech', 'in', 'turkey', 'fort', 'telecom', 'and', 'spaceteam', 'in', 'russia', 'the', 'european', 'market', 'leader', 'tomtom', 'telematics', 'princip', 'in', 'the', 'czech', 'republic', 'the', 'major', 'truck', 'oem', 'scania', 'and', 'secret', 'control', 'which', 'is', 'based', 'in', 'hungary', 'with', 'the', 'exception', 'of', 'tomtom', 'telematics', 'and', 'scania', 'the', 'major', 'international', 'solution', 'providers', 'based', 'in', 'western', 'europe', 'north', 'america', 'or', 'south', 'africa', 'are', 'yet', 'to', 'reach', 'the', 'top', '15', 'list', 'for', 'this', 'region', 'concluded', 'mr', 'andersson', 'publication', 'type', 'newswire', 'copyright', '2017', 'menafn', 'com', 'all', 'rights', 'reserved']\n",
      "____________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_articles = 0\n",
    "regex1 = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "regex2 = re.compile('[%s]' % re.escape(\"\\n\"))\n",
    "regex3 = re.compile('[%s]' % re.escape(string.punctuation + \"\\n\"))\n",
    "\n",
    "for index_f, path in enumerate(paths):\n",
    "    if 500 > index_f > 498:\n",
    "        \n",
    "        articles = set()\n",
    "        \n",
    "        print(datetime.datetime.now())   \n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        with open(path[1], 'rb') as fd:\n",
    "            for parsed_object in ijson.items(fd, \"item\"):\n",
    "                \n",
    "                counter = counter + 1\n",
    "                if counter > 1:\n",
    "                    break\n",
    "                \n",
    "                # Check if the article is in English\n",
    "                try:\n",
    "                    if 'ENGLISH' not in set(parsed_object['language']):\n",
    "                        continue\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                \n",
    "                # Save the object\n",
    "                try:   \n",
    "                    article = parsed_object['content'].lower()\n",
    "                    #words = regex3.sub(' ', article).split()\n",
    "                    if any(keyphrase in article for keyphrase in set([\"of\", \"that\", \"and\"])):\n",
    "                        pass\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    paragraphs = article.split(\"\\n\\n\")\n",
    "                    paragraphs = [regex2.sub('. ', paragraph) for paragraph in paragraphs\n",
    "                                 if regex3.sub(' ', paragraph).split()]\n",
    "                    print(words)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        fd.close()\n",
    "                    \n",
    "                #print(parsed_object['language'][0:100])\n",
    "                print(\"____________________\\n\")\n",
    "                #gg = o['content']\n",
    "\n",
    "        #print(templist.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9936"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies_df = tmp\n",
    "len(k_company_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9914"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_company_df = k_company_df.dropna()\n",
    "companies_list = k_company_df.index.get_values()\n",
    "len(k_company_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k_company_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f630f16e72cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'temp_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'temp_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mk_company_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'temp_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k_company_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mk_company_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'temp_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k_company_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'k_company_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Save for later use with keywords extraction and load\n",
    "if not os.path.exists(os.path.join(basedir, 'temp_data')):\n",
    "    os.makedirs(os.path.join(basedir, 'temp_data'))\n",
    "k_company_df.to_csv(os.path.join(basedir, 'temp_data', 'k_company_df.csv'), sep=\";\")\n",
    "\n",
    "k_company_df = pd.read_csv(os.path.join(basedir, 'temp_data', 'k_company_df.csv'), sep=\";\")\n",
    "k_company_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = pd.DataFrame()\n",
    "num_articles = 0\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index_f, file in enumerate(paths):\n",
    "    #if index_f > 0:\n",
    "    #    break\n",
    "    \n",
    "    print(file)\n",
    "    print(index_f)\n",
    "\n",
    "    try:\n",
    "        articles = pd.read_json(file[1])\n",
    "    except ValueError:\n",
    "        print(\"empty/faulty file\")\n",
    "        continue\n",
    "\n",
    "    if \"content\" not in articles.columns:\n",
    "        print(\"no content\")\n",
    "        continue\n",
    "\n",
    "    articles.loc[:, \"company\"] = file[0]\n",
    "    articles.loc[:, \"uuid\"] = k_company_df.loc[file[0].lower(), \"uuid\"]\n",
    "    articles.loc[:, \"description\"] = k_company_df.loc[file[0].lower(), \"description\"]\n",
    "\n",
    "    articles.dropna(subset=[\"content\"], inplace=True)\n",
    "\n",
    "    texts = [regex.sub('', i).lower() for i in articles.loc[:, \"content\"]]\n",
    "    keyphrases = ast.literal_eval(k_company_df.loc[file[0].lower(), \"kp_topicrank\"])\n",
    "    num_articles = num_articles + len(texts)\n",
    "\n",
    "    # Check whether the any of the keywords exist in the text\n",
    "    bi = [1 \n",
    "             if any([keyphrase in text \n",
    "                     for (keyphrase, prob) in (keyphrases)]) and 250 < len(str.split(text)) < 500 \n",
    "             else 0\n",
    "            for text in texts]\n",
    "\n",
    "    articles.loc[:, \"kp_topicrank_bi\"] = bi\n",
    "    articles.dropna(subset=[\"content\"], inplace=True)\n",
    "\n",
    "    all_articles = all_articles.append(articles, ignore_index=True, sort=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       NaN\n",
       "1       NaN\n",
       "2       NaN\n",
       "3       NaN\n",
       "4       NaN\n",
       "5       NaN\n",
       "6       NaN\n",
       "7       NaN\n",
       "8       NaN\n",
       "9       NaN\n",
       "10      NaN\n",
       "11      NaN\n",
       "12      NaN\n",
       "13      NaN\n",
       "14      NaN\n",
       "15      NaN\n",
       "16      NaN\n",
       "17      NaN\n",
       "18      NaN\n",
       "19      NaN\n",
       "20      NaN\n",
       "21      NaN\n",
       "22      NaN\n",
       "23      NaN\n",
       "24      NaN\n",
       "25      NaN\n",
       "26      NaN\n",
       "27      NaN\n",
       "28      NaN\n",
       "29      NaN\n",
       "       ... \n",
       "5662    0.0\n",
       "5663    0.0\n",
       "5664    0.0\n",
       "5665    1.0\n",
       "5666    1.0\n",
       "5667    0.0\n",
       "5668    0.0\n",
       "5669    0.0\n",
       "5670    0.0\n",
       "5671    0.0\n",
       "5672    0.0\n",
       "5673    0.0\n",
       "5674    0.0\n",
       "5675    0.0\n",
       "5676    0.0\n",
       "5677    0.0\n",
       "5678    0.0\n",
       "5679    0.0\n",
       "5680    0.0\n",
       "5681    0.0\n",
       "5682    0.0\n",
       "5683    0.0\n",
       "5684    0.0\n",
       "5685    0.0\n",
       "5686    0.0\n",
       "5687    0.0\n",
       "5688    0.0\n",
       "5689    1.0\n",
       "5690    1.0\n",
       "5691    0.0\n",
       "Name: kp_topicrank_bi, Length: 5692, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[\"kp_topicrank_bi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% of articles were are containing corresponding keyphrases extracted from the descriptions of the companies\n"
     ]
    }
   ],
   "source": [
    "print(str(len(all_articles/num_articles)) + \"% of articles were are containing corresponding keyphrases extracted from the descriptions of the companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "if not os.path.exists(os.path.join(basedir, 'temp_data')):\n",
    "    os.makedirs(os.path.join(basedir, 'temp_data'))\n",
    "all_articles.to_csv(os.path.join(basedir, 'temp_data', 'all_articles.csv'), sep=\";\")\n",
    "all_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABRA', 'ACRONIS', 'ACTON', 'AIRSTONE', 'ALBERT', 'ALEA', 'ALIANZA', 'AMARA', 'ANDO', 'ARKIN', 'ARTSPACE', 'ASANA', 'ASOKA', 'ASTRID', 'ATHOS', 'AVA', 'AYR', 'BACKDOOR', 'BANNERMAN', 'BARKLY', 'BASTILLE', 'BBB', 'BEBO', 'BENTO', 'BIA', 'BINDO', 'BIOME', 'BIRDBOX', 'BITTORRENT', 'BIX', 'BIZEN', 'BOARDVOTE', 'BONOBOS', 'BOTTLENOSE', 'BOWERY', 'BRAINTREE', 'BRIT', 'BUDDYTV', 'CAIS', 'CALERA', 'CALVIN', 'CARTESIAN', 'CASPER', 'CASSATT', 'CENTRO', 'CERBERUS', 'CHAI', 'CLARK', 'CLEO', 'CLICKABLE', 'CLOE', 'COTOPAXI', 'CRONO', 'CROSSFADER', 'DAILYMOTION', 'DELOS', 'DIGBY', 'DIY', 'DOO', 'DOPPELGANGER', 'ECO', 'ELASTICA', 'ELEMENTUM', 'ELLIE', 'ELɘ', 'EMMA', 'ENDORPHIN', 'EPIPHYTE', 'EPS', 'ESS', 'EVERLY', 'EXCALIBUR', 'EXOGENESIS', 'EYESPOT', 'FAB', 'FACEBOOK', 'FANBASE', 'FARADAY', 'FIDELIS', 'FLASHPOINT', 'FLAVOUR', 'FLYER', 'FLYNN', 'FONDU', 'FORMSPRING', 'FRONTO', 'FRS', 'FULHAM', 'FUZE', 'FY', 'GAMETIME', 'GAMEZONE', 'GAMGEE', 'GELI', 'GENI', 'GOLDSTAR', 'GOLGI', 'GORGIAS', 'GRO', 'HALLUX', 'HARA', 'HEDVIG', 'HELIOS', 'HOMIE', 'HOUDINI', 'IGG', 'IGOR', 'IMVU', 'INTELLIVISION', 'JANA', 'JETHRO', 'JOLIE', 'JOOST', 'JOSEPHINE', 'JUNE', 'KAHUNA', 'KASH', 'KAT', 'KATO', 'KENNA', 'KIKO', 'KIVA', 'KLARA', 'KNO', 'KRAFTWERK', 'KRAK', 'KUDO', 'KUNA', 'KWIK', 'LALA', 'LEET', 'LEO', 'LEVERTON', 'LIA', 'LIFECYCLE', 'LIFESIZE', 'LIMA', 'LOLA', 'LOUP', 'LUA', 'LUKA', 'LULLY', 'LUMA', 'LUMI', 'LUMINOSO', 'LUX', 'MARIANA', 'MARKHOR', 'MASALA', 'MATCHA', 'MAZ', 'MEDIO', 'MELBA', 'METADATA', 'MIKA', 'MILA', 'MILO', 'MIRA', 'MISO', 'MITRO', 'MODELO', 'MOLI', 'MOSHI', 'MOWGLI', 'MPV', 'MYCROFT', 'MYSPACE', 'NAV', 'NELLO', 'NETBOOKS', 'NETMINDER', 'NEWCO', 'NICO', 'NITRO', 'NOM', 'NORSE', 'NOVI', 'NULATO', 'OCTAVIAN', 'OCULUS', 'OFFLINE', 'OLLIE', 'ONO', 'OPER', 'ORCA', 'OSCAR', 'OSMO', 'OSSIA', 'OUTRO', 'OVALIS', 'PANA', 'PARSABLE', 'PELOTON', 'PERSEUS', 'PHENOM', 'PHILO', 'PIPEFISH', 'PIRC', 'PLAYDATE', 'PLAYLIST', 'PLEX', 'POPUP', 'PUTNEY', 'RAMEN', 'RC', 'RECURVE', 'REDDIT', 'REDOX', 'RIDLEY', 'RIVA', 'RIZE', 'RIZM', 'ROBBY', 'ROKU', 'RPO', 'SAMSARA', 'SANO', 'SEATERS', 'SEGOVIA', 'SHIPPO', 'SHOTO', 'SIGFIG', 'SIRI', 'SNOWDON', 'SOCI', 'SOCRATIC', 'SOLEY', 'SPOCK', 'STRATOS', 'STUMBLEUPON', 'SUNFIRE', 'SUPERFLY', 'SZL', 'TALKSPACE', 'TECHNORATI', 'TELEPORT', 'TENJIN', 'TRAVIS', 'TREEHOUSE', 'TREVENA', 'TUUL', 'TWOFISH', 'TYCHE', 'UBER', 'UMAMI', 'UNDERSTORY', 'UNU', 'UNUM', 'VAYU', 'VENGA', 'VENIAM', 'VERA', 'VERT', 'VIDA', 'VIDI', 'VIGO', 'VINCI', 'VING', 'VIV', 'VUE', 'WALDO', 'WASABI', 'WHIPTAIL', 'WIKIA', 'WILDCARD', 'WINNIE', 'WUNDER', 'XING', 'YANTRA', 'YELLO', 'YORN', 'YOSHI', 'YOUTUBE', 'YUME', 'ZAZA', 'ZEEBO', 'ZEO', 'ZOLA', 'ZOOSK', 'ZORA', 'ZUKI']\n"
     ]
    }
   ],
   "source": [
    "print(companies_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76920"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNPARSED</th>\n",
       "      <th>autor</th>\n",
       "      <th>byline</th>\n",
       "      <th>contact</th>\n",
       "      <th>content</th>\n",
       "      <th>dateline</th>\n",
       "      <th>distribution</th>\n",
       "      <th>error</th>\n",
       "      <th>error_code</th>\n",
       "      <th>grafic</th>\n",
       "      <th>...</th>\n",
       "      <th>pub_type</th>\n",
       "      <th>rubrik</th>\n",
       "      <th>section</th>\n",
       "      <th>series</th>\n",
       "      <th>source</th>\n",
       "      <th>company</th>\n",
       "      <th>uuid</th>\n",
       "      <th>description</th>\n",
       "      <th>country</th>\n",
       "      <th>correction_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4395</th>\n",
       "      <td>AFX - Asia\\n\\n                     November 2,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AFX - Asia\\n\\n                     November 2,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>musx</td>\n",
       "      <td>9d2a0961-a4dc-ae6e-886b-64246ca3f4ab</td>\n",
       "      <td>Whenever you first meet someone, there is the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>AFX - Asia\\n\\n                      August 16,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AFX - Asia\\n\\n                      August 16,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>musx</td>\n",
       "      <td>9d2a0961-a4dc-ae6e-886b-64246ca3f4ab</td>\n",
       "      <td>Whenever you first meet someone, there is the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>Manila Times\\n\\n                             J...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manila Times\\n\\n                             J...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>musx</td>\n",
       "      <td>9d2a0961-a4dc-ae6e-886b-64246ca3f4ab</td>\n",
       "      <td>Whenever you first meet someone, there is the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>AFX - Asia\\n\\n                       May 24, 2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AFX - Asia\\n\\n                       May 24, 2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>musx</td>\n",
       "      <td>9d2a0961-a4dc-ae6e-886b-64246ca3f4ab</td>\n",
       "      <td>Whenever you first meet someone, there is the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>AFX - Asia\\n\\n                       April 22,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AFX - Asia\\n\\n                       April 22,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>musx</td>\n",
       "      <td>9d2a0961-a4dc-ae6e-886b-64246ca3f4ab</td>\n",
       "      <td>Whenever you first meet someone, there is the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               UNPARSED autor byline contact  \\\n",
       "4395  AFX - Asia\\n\\n                     November 2,...   NaN    NaN     NaN   \n",
       "4396  AFX - Asia\\n\\n                      August 16,...   NaN    NaN     NaN   \n",
       "4397  Manila Times\\n\\n                             J...   NaN    NaN     NaN   \n",
       "4398  AFX - Asia\\n\\n                       May 24, 2...   NaN    NaN     NaN   \n",
       "4399  AFX - Asia\\n\\n                       April 22,...   NaN    NaN     NaN   \n",
       "\n",
       "                                                content dateline distribution  \\\n",
       "4395  AFX - Asia\\n\\n                     November 2,...      NaN          NaN   \n",
       "4396  AFX - Asia\\n\\n                      August 16,...      NaN          NaN   \n",
       "4397  Manila Times\\n\\n                             J...      NaN          NaN   \n",
       "4398  AFX - Asia\\n\\n                       May 24, 2...      NaN          NaN   \n",
       "4399  AFX - Asia\\n\\n                       April 22,...      NaN          NaN   \n",
       "\n",
       "     error  error_code grafic       ...       pub_type rubrik section series  \\\n",
       "4395   NaN         NaN    NaN       ...            NaN    NaN     NaN    NaN   \n",
       "4396   NaN         NaN    NaN       ...            NaN    NaN     NaN    NaN   \n",
       "4397   NaN         NaN    NaN       ...            NaN    NaN     NaN    NaN   \n",
       "4398   NaN         NaN    NaN       ...            NaN    NaN     NaN    NaN   \n",
       "4399   NaN         NaN    NaN       ...            NaN    NaN     NaN    NaN   \n",
       "\n",
       "     source company                                  uuid  \\\n",
       "4395    NaN    musx  9d2a0961-a4dc-ae6e-886b-64246ca3f4ab   \n",
       "4396    NaN    musx  9d2a0961-a4dc-ae6e-886b-64246ca3f4ab   \n",
       "4397    NaN    musx  9d2a0961-a4dc-ae6e-886b-64246ca3f4ab   \n",
       "4398    NaN    musx  9d2a0961-a4dc-ae6e-886b-64246ca3f4ab   \n",
       "4399    NaN    musx  9d2a0961-a4dc-ae6e-886b-64246ca3f4ab   \n",
       "\n",
       "                                            description country  \\\n",
       "4395  Whenever you first meet someone, there is the ...     NaN   \n",
       "4396  Whenever you first meet someone, there is the ...     NaN   \n",
       "4397  Whenever you first meet someone, there is the ...     NaN   \n",
       "4398  Whenever you first meet someone, there is the ...     NaN   \n",
       "4399  Whenever you first meet someone, there is the ...     NaN   \n",
       "\n",
       "     correction_date  \n",
       "4395             NaN  \n",
       "4396             NaN  \n",
       "4397             NaN  \n",
       "4398             NaN  \n",
       "4399             NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
